{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import io\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('io.hdf.default_format','table')\n",
    "\n",
    "filepath1 =  'hdf5.test.1.h5'\n",
    "filepath2 =  'hdf5.test.2.h5'\n",
    "filepath3 =  'hdf5.test.3.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get dataset properties as ndarray properties\n",
      "(8, 8) 64\n",
      "Index ndarray\n",
      "63 [56 57 58 59 60 61 62 63]\n",
      "Get the entire ndarray\n",
      "<class 'numpy.ndarray'>\n",
      "[[ 0  1  2  3  4  5  6  7]\n",
      " [ 8  9 10 11 12 13 14 15]\n",
      " [16 17 18 19 20 21 22 23]\n",
      " [24 25 26 27 28 29 30 31]\n",
      " [32 33 34 35 36 37 38 39]\n",
      " [40 41 42 43 44 45 46 47]\n",
      " [48 49 50 51 52 53 54 55]\n",
      " [56 57 58 59 60 61 62 63]]\n"
     ]
    }
   ],
   "source": [
    "# Accessing ndarray data in H5 dataset.\n",
    "A = np.arange(64).reshape(8,8)\n",
    "\n",
    "with io.BytesIO() as bio:\n",
    "    # Using in-memory file to mock file object\n",
    "    with h5py.File(bio, 'w') as f:\n",
    "        f['dataset'] = A\n",
    "        print(\"Get dataset properties as ndarray properties\")\n",
    "        print(f['dataset'].shape, f['dataset'].size)\n",
    "        print(\"Index ndarray\")\n",
    "        print(f['dataset'][-1,-1], f['dataset'][-1])\n",
    "        print(\"Get the entire ndarray\")\n",
    "        Ap = f['dataset'][()]\n",
    "        print(type(Ap))\n",
    "        print(Ap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing/reading ndarray and DataFrame to/from H5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h5 file's keys:  test0\n",
      "[[0 1 2]\n",
      " [2 3 1]\n",
      " [1 3 4]]\n",
      "   patch_size  magnification dataset_name\n",
      "0        1024             10           tn\n",
      "1        1024             20     ovr_mmrd\n",
      "2         512             40         mmrd\n",
      "\n",
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: hdf5.test.1.h5\n",
      "/gp1/test2            frame_table  (typ->appendable,nrows->2,ncols->3,indexers->[index],dc->[])\n",
      "/test1                frame_table  (typ->appendable,nrows->3,ncols->3,indexers->[index],dc->[])\n",
      "\n",
      "   patch_size  magnification dataset_name\n",
      "0        2048             80           tn\n",
      "1          64              5          ovr\n",
      "\n",
      "   patch_size  magnification dataset_name\n",
      "0        1024             10           tn\n",
      "1        1024             20     ovr_mmrd\n",
      "2         512             40         mmrd\n",
      "[[0 1 2]\n",
      " [2 3 1]\n",
      " [1 3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Documentation on pandas.HDFStore:\n",
    "# https://www.kite.com/python/docs/pandas.HDFStore\n",
    "\n",
    "# save ndarray to H5 file\n",
    "a = np.array([\n",
    "    [0,1,2],\n",
    "    [2,3,1],\n",
    "    [1,3,4]])\n",
    "# write a H5 file\n",
    "with h5py.File(filepath1, 'w') as f:\n",
    "    # create a dataset\n",
    "    f.create_dataset('test0', data=a)\n",
    "# read a H5 file\n",
    "with h5py.File(filepath1, 'r') as f:\n",
    "    print(\"h5 file's keys: \", *f.keys())\n",
    "    # get h5py.Dataset\n",
    "    dset = f['test0']\n",
    "    # get ndarray from h5py.Dataset\n",
    "    ap = dset[()]\n",
    "    print(ap)\n",
    "    \n",
    "# save pd.DataFrame to H5 file using pd.HDFStore\n",
    "d = {'patch_size': [1024, 1024, 512],\n",
    "     'magnification': [10, 20, 40],\n",
    "     'dataset_name': ['tn', 'ovr_mmrd', 'mmrd']}\n",
    "df = pd.DataFrame(d)\n",
    "with pd.HDFStore(filepath1) as s:\n",
    "    s['test1'] = df\n",
    "\n",
    "# must use pd.HDFStore to read DataFrame data\n",
    "with pd.HDFStore(filepath1, 'r') as s:\n",
    "    print(s['test1'])\n",
    "\n",
    "# adding pd.DataFrame to H5 file using pd.HDFStore needs 'a' mode.\n",
    "d = {'patch_size': [2048, 64],\n",
    "     'magnification': [80, 5],\n",
    "     'dataset_name': ['tn', 'ovr']}\n",
    "df = pd.DataFrame(d)\n",
    "with pd.HDFStore(filepath1, 'a') as s:\n",
    "    s['gp1/test2'] = df\n",
    "\n",
    "# get info and pd.DataFrame values from H5 \n",
    "with pd.HDFStore(filepath1) as s:\n",
    "    print()\n",
    "    print(s.info())\n",
    "    print()\n",
    "    print(s['gp1/test2'])\n",
    "    print()\n",
    "    print(s['test1'])\n",
    "\n",
    "# must use h5py.File to read ndarray data\n",
    "with h5py.File(filepath1, 'r') as f:\n",
    "    print(f['test0'][()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patch_size  magnification dataset_name\n",
      "0        1024             10           tn\n",
      "1        1024             20     ovr_mmrd\n",
      "2         512             40         mmrd\n",
      "0        2048             80           tn\n",
      "1          64              5          ovr\n"
     ]
    }
   ],
   "source": [
    "# appending to an existing dataframe\n",
    "# Need to set `pd.set_option('io.hdf.default_format','table')` to append a dataframe\n",
    "# To not overwrite, pd.HDFStore must be constructed with the append 'a' mode.\n",
    "# Based on:\n",
    "# https://www.kite.com/python/docs/pandas.HDFStore\n",
    "# https://stackoverflow.com/questions/39638179/pandas-hdf5-append-time-series-fails\n",
    "\n",
    "d = {'patch_size': [1024, 1024, 512],\n",
    "     'magnification': [10, 20, 40],\n",
    "     'dataset_name': ['tn', 'ovr_mmrd', 'mmrd']}\n",
    "df1 = pd.DataFrame(d)\n",
    "d = {'patch_size': [2048, 64],\n",
    "     'magnification': [80, 5],\n",
    "     'dataset_name': ['tn', 'ovr']}\n",
    "df2 = pd.DataFrame(d)\n",
    "\n",
    "with pd.HDFStore(filepath1, 'w') as s:\n",
    "    s.put('test', df1, format='t', append=True, data_columns=True)\n",
    "    # s.append('test', df1)\n",
    "with pd.HDFStore(filepath1, 'a') as s:\n",
    "    s.put('test', df2, format='t', append=True, data_columns=True)\n",
    "    # s.append('test', df2)\n",
    "with pd.HDFStore(filepath1, 'a') as s:\n",
    "    s['test'].reset_index(drop=True, inplace=True)\n",
    "with pd.HDFStore(filepath1, 'r') as s:\n",
    "    print(s['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Unable to open object (component not found)'\n",
      "h5 file's keys:  gr1\n",
      "gr1 in f? True\n",
      "gr1/gr2 in f? True\n",
      "gr1/gr2/test0 in f? True\n",
      "gr1/gr2/test0 is dataset? True\n",
      "gr1/gr2/test0 is group? False\n",
      "[[0 1 2]\n",
      " [2 3 1]\n",
      " [1 3 4]]\n",
      "test0 in g? True\n",
      "test0 is dataset? True\n"
     ]
    }
   ],
   "source": [
    "# save ndarray to H5 file\n",
    "a = np.array([\n",
    "    [0,1,2],\n",
    "    [2,3,1],\n",
    "    [1,3,4]])\n",
    "# write a H5 file\n",
    "with h5py.File(filepath2, 'w') as f:\n",
    "    try:\n",
    "        # can't access groups or datasets that have not been created yet\n",
    "        f['gr1/gr2/test0']\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    # create a group\n",
    "    gr = f.create_group(\"gr1/gr2\")\n",
    "    # create a dataset\n",
    "    gr.create_dataset('test0', data=a)\n",
    "    \n",
    "# read a H5 file\n",
    "with h5py.File(filepath2, 'r') as f:\n",
    "    print(\"h5 file's keys: \", *f.keys())\n",
    "    print(\"gr1 in f?\", 'gr1' in f)\n",
    "    print(\"gr1/gr2 in f?\", 'gr1/gr2' in f)\n",
    "    print(\"gr1/gr2/test0 in f?\", 'gr1/gr2/test0' in f)\n",
    "    print(\"gr1/gr2/test0 is dataset?\", isinstance(f['gr1/gr2/test0'], h5py.Dataset))\n",
    "    print(\"gr1/gr2/test0 is group?\", isinstance(f['gr1/gr2/test0'], h5py.Group))\n",
    "    # get h5py.Dataset\n",
    "    dset = f['gr1/gr2/test0']\n",
    "    # get ndarray from h5py.Dataset\n",
    "    ap = dset[()]\n",
    "    print(ap)\n",
    "    group = f['gr1/gr2']\n",
    "    print(\"test0 in g?\", 'test0' in group)\n",
    "    print(\"test0 is dataset?\", isinstance(group['test0'], h5py.Dataset))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  2  3  1]\n",
      " [ 0 -1 -1  1]\n",
      " [ 0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# save ndarray to H5 file\n",
    "width = 4\n",
    "height = 3\n",
    "# write a H5 file\n",
    "with h5py.File(filepath2, 'w') as f:\n",
    "    f.create_dataset('test', (height, width,), dtype='i')\n",
    "with h5py.File(filepath2, 'a') as f:\n",
    "    dset = f['test']\n",
    "    # write array horizontally\n",
    "    dset[1,1:3] = np.array([-1,-1])\n",
    "    # write array vertically\n",
    "    dset[0:2,3] = np.array([1,1])\n",
    "    # write number\n",
    "    dset[0,1] = 2\n",
    "    # write a single cell\n",
    "    dset[0,2] = np.array([3])\n",
    "with h5py.File(filepath2, 'r') as f:\n",
    "    dset = f['test']\n",
    "    print(dset[()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to create group (name already exists)\n",
      "\n",
      "get the IDs in h5 file\n",
      "gr11/gr21/a1\n",
      "gr11/gr22/a2\n",
      "gr12/gr2/a3\n",
      "gr13/gr2/gr3/a4\n",
      "\n",
      "walk words in h5 file\n",
      "gr11\n",
      "  gr21\n",
      "     a1\n",
      "  gr22\n",
      "     a2\n",
      "gr12\n",
      "  gr2\n",
      "     a3\n",
      "gr13\n",
      "  gr2\n",
      "     gr3\n",
      "       a4\n",
      "\n",
      "Visit all groups and datasets in file\n",
      "gr11       <HDF5 group \"/gr11\" (2 members)>\n",
      "gr11/gr21       <HDF5 group \"/gr11/gr21\" (1 members)>\n",
      "gr11/gr21/a1       <HDF5 dataset \"a1\": shape (2, 2), type \"<i8\">\n",
      "gr11/gr22       <HDF5 group \"/gr11/gr22\" (1 members)>\n",
      "gr11/gr22/a2       <HDF5 dataset \"a2\": shape (2, 2), type \"<i8\">\n",
      "gr12       <HDF5 group \"/gr12\" (1 members)>\n",
      "gr12/gr2       <HDF5 group \"/gr12/gr2\" (1 members)>\n",
      "gr12/gr2/a3       <HDF5 dataset \"a3\": shape (2, 2), type \"<i8\">\n",
      "gr13       <HDF5 group \"/gr13\" (1 members)>\n",
      "gr13/gr2       <HDF5 group \"/gr13/gr2\" (1 members)>\n",
      "gr13/gr2/gr3       <HDF5 group \"/gr13/gr2/gr3\" (1 members)>\n",
      "gr13/gr2/gr3/a4       <HDF5 dataset \"a4\": shape (2, 2), type \"<i8\">\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([\n",
    "    [0,1],\n",
    "    [2,3]])\n",
    "a2 = np.array([\n",
    "    [1,0],\n",
    "    [0,1]])\n",
    "a3 = np.array([\n",
    "    [0,1],\n",
    "    [1,0]])\n",
    "a4 = np.array([\n",
    "    [1,-1],\n",
    "    [-1,1]])\n",
    "\n",
    "with h5py.File(filepath3, 'w') as f:\n",
    "    gr = f.create_group(\"gr11/gr21\")\n",
    "    dset = gr.create_dataset('a1', data=a1)\n",
    "    # cannot create group gr11 since already exist\n",
    "    # but can create group gr11/gr22 since diff. inner group\n",
    "    try:\n",
    "        gr = f.create_group('gr11')\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "    gr = f.create_group(\"gr11/gr22\")\n",
    "    gr.create_dataset('a2', data=a2)\n",
    "    # can create dataset directly within groups\n",
    "    dset = f.create_dataset('gr12/gr2/a3', data=a3)\n",
    "    dset = f.create_dataset('gr13/gr2/gr3/a4', data=a4)\n",
    "\n",
    "def get_h5_keys(f, key=''):\n",
    "    keys = [ ]\n",
    "    if isinstance(f, h5py.Dataset):\n",
    "        return [key]\n",
    "    else:\n",
    "        for k, v in f.items():\n",
    "            if key:\n",
    "                tmp_key = f\"{key}/{k}\"\n",
    "            else:\n",
    "                tmp_key = k\n",
    "            tmp_keys = get_h5_keys(v, key=tmp_key)\n",
    "            keys.extend(tmp_keys)\n",
    "        return keys\n",
    "\n",
    "print(\"\\nget the IDs in h5 file\")\n",
    "with h5py.File(filepath3, 'r') as f:\n",
    "    for k in get_h5_keys(f):\n",
    "        print(k)\n",
    "    \n",
    "print(\"\\nwalk words in h5 file\")\n",
    "with h5py.File(filepath3, 'r') as f:\n",
    "    for k1, v1 in f.items():\n",
    "        print(k1)\n",
    "        for k2, v2 in v1.items():\n",
    "            print(' ', k2)\n",
    "            for k3, v3 in v2.items():\n",
    "                print('    ', k3)\n",
    "                if isinstance(v3, h5py.Group):\n",
    "                    for k4, v4 in v3.items():\n",
    "                        print('      ', k4)\n",
    "\n",
    "print(\"\\nVisit all groups and datasets in file\")\n",
    "with h5py.File(filepath3, 'r') as f:\n",
    "    f.visititems(lambda _name, _obj: print(_name, '     ', _obj))\n",
    "    # alernatively use the below to print the IDs\n",
    "    # f.visit(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
