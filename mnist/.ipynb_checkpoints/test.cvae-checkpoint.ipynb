{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cross-defendant",
   "metadata": {},
   "source": [
    "Conditional Variational Auto-encoder\n",
    "\n",
    "Linear operations apply affine transform $y = x A^T + b$. Documentation for `torch.nn.Linear()`:  \n",
    "<https://pytorch.org/docs/stable/generated/torch.nn.Linear.html>\n",
    "\n",
    "Based on code from:  \n",
    "<https://github.com/timbmg/VAE-CVAE-MNIST>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "assisted-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "concerned-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2onehot(idx, n):\n",
    "    assert torch.max(idx).item() < n\n",
    "    if idx.dim() == 1:\n",
    "        idx = idx.unsqueeze(1)\n",
    "    onehot = torch.zeros(idx.size(0), n).to(idx.device)\n",
    "    onehot.scatter_(1, idx, 1)\n",
    "    return onehot\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder_layer_sizes, latent_size, decoder_layer_sizes,\n",
    "                 conditional=False, num_labels=0):\n",
    "        super().__init__()\n",
    "        if conditional:\n",
    "            assert num_labels > 0\n",
    "        assert type(encoder_layer_sizes) == list\n",
    "        assert type(latent_size) == int\n",
    "        assert type(decoder_layer_sizes) == list\n",
    "        self.latent_size = latent_size\n",
    "        self.encoder = Encoder(\n",
    "            encoder_layer_sizes, latent_size, conditional, num_labels)\n",
    "        self.decoder = Decoder(\n",
    "            decoder_layer_sizes, latent_size, conditional, num_labels)\n",
    "\n",
    "    def forward(self, x, c=None):\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(-1, 28*28)\n",
    "        means, log_var = self.encoder(x, c)\n",
    "        z = self.reparameterize(means, log_var)\n",
    "        recon_x = self.decoder(z, c)\n",
    "        return recon_x, means, log_var, z\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def inference(self, z, c=None):\n",
    "        recon_x = self.decoder(z, c)\n",
    "        return recon_x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer_sizes, latent_size, conditional, num_labels):\n",
    "        super().__init__()\n",
    "        self.conditional = conditional\n",
    "        if self.conditional:\n",
    "            layer_sizes[0] += num_labels\n",
    "        self.MLP = nn.Sequential()\n",
    "        for i, (in_size, out_size) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "            self.MLP.add_module(\n",
    "                name=\"L{:d}\".format(i), module=nn.Linear(in_size, out_size))\n",
    "            self.MLP.add_module(name=\"A{:d}\".format(i), module=nn.ReLU())\n",
    "        self.linear_means = nn.Linear(layer_sizes[-1], latent_size)\n",
    "        self.linear_log_var = nn.Linear(layer_sizes[-1], latent_size)\n",
    "\n",
    "    def forward(self, x, c=None):\n",
    "        if self.conditional:\n",
    "            c = idx2onehot(c, n=10)\n",
    "            x = torch.cat((x, c), dim=-1)\n",
    "        x = self.MLP(x)\n",
    "        means = self.linear_means(x)\n",
    "        log_vars = self.linear_log_var(x)\n",
    "        return means, log_vars\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer_sizes, latent_size, conditional, num_labels):\n",
    "        super().__init__()\n",
    "        self.MLP = nn.Sequential()\n",
    "        self.conditional = conditional\n",
    "        if self.conditional:\n",
    "            input_size = latent_size + num_labels\n",
    "        else:\n",
    "            input_size = latent_size\n",
    "        for i, (in_size, out_size) in enumerate(zip([input_size]+layer_sizes[:-1], layer_sizes)):\n",
    "            self.MLP.add_module(\n",
    "                name=\"L{:d}\".format(i), module=nn.Linear(in_size, out_size))\n",
    "            if i+1 < len(layer_sizes):\n",
    "                self.MLP.add_module(name=\"A{:d}\".format(i), module=nn.ReLU())\n",
    "            else:\n",
    "                self.MLP.add_module(name=\"sigmoid\", module=nn.Sigmoid())\n",
    "\n",
    "    def forward(self, z, c):\n",
    "        if self.conditional:\n",
    "            c = idx2onehot(c, n=10)\n",
    "            z = torch.cat((z, c), dim=-1)\n",
    "        x = self.MLP(z)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "painful-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = AttrDict(seed=0, epochs=10, batch_size=64, learning_rate=0.001,\n",
    "        encoder_layer_sizes=[784, 256], decoder_layer_sizes=[256, 784],\n",
    "        latent_size=2, print_every=100, fig_root='figs',\n",
    "        conditional=True)\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "ts = time.time()\n",
    "\n",
    "dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', transform=torchvision.transforms.ToTensor(), download=True)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "def loss_fn(recon_x, x, mean, log_var):\n",
    "    BCE = torch.nn.functional.binary_cross_entropy(\n",
    "        recon_x.view(-1, 28*28), x.view(-1, 28*28), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "\n",
    "    return (BCE + KLD) / x.size(0)\n",
    "\n",
    "vae = VAE(\n",
    "        encoder_layer_sizes=args.encoder_layer_sizes,\n",
    "        latent_size=args.latent_size,\n",
    "        decoder_layer_sizes=args.decoder_layer_sizes,\n",
    "        conditional=args.conditional,\n",
    "        num_labels=10 if args.conditional else 0).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=args.learning_rate)\n",
    "\n",
    "logs = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "thermal-friday",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   0%|          | 0/938 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0877f981d752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'Training Epoch {epoch}: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     for iteration, (x, y) in tqdm(data_loader, desc=prefix, \n\u001b[0;32m----> 5\u001b[0;31m             dynamic_ncols=True, leave=True, position=0):\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconditional\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    tracker_epoch = defaultdict(lambda: defaultdict(dict))\n",
    "    prefix = f'Training Epoch {epoch}: '\n",
    "    for iteration, (x, y) in enumerate(tqdm(data_loader, desc=prefix, \n",
    "            dynamic_ncols=True, leave=True, position=0)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        if args.conditional:\n",
    "            recon_x, mean, log_var, z = vae(x, y)\n",
    "        else:\n",
    "            recon_x, mean, log_var, z = vae(x)\n",
    "        for i, yi in enumerate(y):\n",
    "            id = len(tracker_epoch)\n",
    "            tracker_epoch[id]['x'] = z[i, 0].item()\n",
    "            tracker_epoch[id]['y'] = z[i, 1].item()\n",
    "            tracker_epoch[id]['label'] = yi.item()\n",
    "        loss = loss_fn(recon_x, x, mean, log_var)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        logs['loss'].append(loss.item())\n",
    "        if iteration % args.print_every == 0 or iteration == len(data_loader)-1:\n",
    "            print(\"Epoch {:02d}/{:02d} Batch {:04d}/{:d}, Loss {:9.4f}\".format(\n",
    "                epoch, args.epochs, iteration, len(data_loader)-1, loss.item()))\n",
    "            if args.conditional:\n",
    "                c = torch.arange(0, 10).long().unsqueeze(1).to(device)\n",
    "                z = torch.randn([c.size(0), args.latent_size]).to(device)\n",
    "                x = vae.inference(z, c=c)\n",
    "            else:\n",
    "                z = torch.randn([10, args.latent_size]).to(device)\n",
    "                x = vae.inference(z)\n",
    "            plt.figure()\n",
    "            plt.figure(figsize=(5, 10))\n",
    "            for p in range(10):\n",
    "                plt.subplot(5, 2, p+1)\n",
    "                if args.conditional:\n",
    "                    plt.text(\n",
    "                        0, 0, \"c={:d}\".format(c[p].item()), color='black',\n",
    "                        backgroundcolor='white', fontsize=8)\n",
    "                plt.imshow(x[p].view(28, 28).cpu().data.numpy())\n",
    "                plt.axis('off')\n",
    "            if not os.path.exists(os.path.join(args.fig_root, str(ts))):\n",
    "                if not(os.path.exists(os.path.join(args.fig_root))):\n",
    "                    os.mkdir(os.path.join(args.fig_root))\n",
    "                os.mkdir(os.path.join(args.fig_root, str(ts)))\n",
    "            plt.savefig(\n",
    "                os.path.join(args.fig_root, str(ts),\n",
    "                             \"E{:d}I{:d}.png\".format(epoch, iteration)),\n",
    "                dpi=300)\n",
    "            plt.clf()\n",
    "            plt.close('all')\n",
    "    df = pd.DataFrame.from_dict(tracker_epoch, orient='index')\n",
    "    g = sns.lmplot(\n",
    "        x='x', y='y', hue='label', data=df.groupby('label').head(100),\n",
    "        fit_reg=False, legend=True)\n",
    "    g.savefig(os.path.join(\n",
    "        args.fig_root, str(ts), \"E{:d}-Dist.png\".format(epoch)),\n",
    "        dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-gender",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
