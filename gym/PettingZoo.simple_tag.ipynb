{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50130d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import enum\n",
    "\n",
    "import matplotlib.pyplot as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.utils import random_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7724bfe",
   "metadata": {},
   "source": [
    "Arguments in instantiate environment.\n",
    "\n",
    "- num_good: number of good agents\n",
    "- num_adversaries: number of adversaries\n",
    "- num_obstacles: number of obstacles\n",
    "- max_cycles: number of frames (a step for each agent) until game terminates\n",
    "- continuous_actions: Whether agent action spaces are discrete(default) or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9858b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=3,\n",
    "    num_adversaries=3,\n",
    "    num_obstacles=2,\n",
    "    max_cycles=100,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cabc86",
   "metadata": {},
   "source": [
    "Adversaries (red) capture non-adversary (green). The map is a 2D grid and everything is initialized in the region [-1, +1]. There doesn't seem to be position clipping for out of bounds, but non-adversary agent are penalized for out of bounds.\n",
    "Agent's observation is a ndarray vector of concatenated data in the following order:\n",
    "\n",
    "1. current velocity (2,)\n",
    "2. current position (2,)\n",
    "3. relative position (2,) of each landmark\n",
    "4. relative position (2,) of each other agent\n",
    "5. velocity (2,) of each other non-adversary agent\n",
    "\n",
    "When there are 3 adverseries and 3 non-adversaries, then advarsary observation space is 24 dimensional and non-advarsary observation space is 22 dimensional.\n",
    "\n",
    "The environment is sequential. Agents move one at a time. Agents are either `adversary_*` for adversary or `agent_*` for non-adversary.\n",
    "\n",
    "Actions:\n",
    "\n",
    "- 0 is NOP\n",
    "- 1 is go left\n",
    "- 2 is go right\n",
    "- 3 is go down\n",
    "- 4 is go up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bc35a8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size (138,)\n",
      "Name of current agent adversary_0\n",
      "Observation space of current agent (24,)\n",
      "Action space of current agent Discrete(5)\n",
      "Sample random action from current agent 1\n",
      "The agent names: adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n",
      "\n",
      "Box([-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf], (24,), float32)\n",
      "\n",
      "agent's name is adversary_0\n",
      "agent's position and velocity coordinates [0. 0.] [ 0.91148203 -0.05041044]\n",
      "is agent an adversary? True\n",
      "landmark's name is landmark 0\n",
      "landmark's position coordinates (doesn't move) [ 0.73009034 -0.75464927]\n"
     ]
    }
   ],
   "source": [
    "# Print variables of the environment\n",
    "# Documentation:   https://www.pettingzoo.ml/api\n",
    "env.reset()\n",
    "print(\"State size\", env.state_space.shape)\n",
    "print(\"Name of current agent\", env.agent_selection)\n",
    "print(\"Observation space of current agent\", env.observation_space(env.agent_selection).shape)\n",
    "print(\"Action space of current agent\", env.action_space(env.agent_selection))\n",
    "print(\"Sample random action from current agent\", env.action_space(env.agent_selection).sample())\n",
    "print(\"The agent names:\", *env.agents)\n",
    "print()\n",
    "\n",
    "print()\n",
    "print()\n",
    "\n",
    "# select an agent in the environment world, after using env.unwrapped\n",
    "agent = env.world.agents[0]\n",
    "print(\"agent's name is\", agent.name)\n",
    "print(\"agent's position and velocity coordinates\", agent.state.p_vel, agent.state.p_pos)\n",
    "print(\"is agent an adversary?\", agent.adversary)\n",
    "\n",
    "landmark = env.world.landmarks[0]\n",
    "print(\"landmark's name is\", landmark.name)\n",
    "print(\"landmark's position coordinates (doesn't move)\", landmark.state.p_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f3522ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward -402.272556480898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2011.36278240449"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo environment with random policy\n",
    "env.reset()\n",
    "random_demo(env, render=True, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33232068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hardcode_policy(observation, agent):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    agent : str\n",
    "    \"\"\"\n",
    "#     print(observation.shape)\n",
    "#     print(agent)\n",
    "    if \"adversary\" in agent:\n",
    "        # adversary\n",
    "        if agent == \"adversary_0\":\n",
    "            return 4\n",
    "        \n",
    "    if \"agent\" in agent:\n",
    "        # non-adversary\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy(observation, agent)\n",
    "        env.step(action)\n",
    "    # time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6301c6a",
   "metadata": {},
   "source": [
    "Messages are of size 5?\n",
    "\n",
    "Concatenate the messages from all the actors and add them to the message input for the current agent.\n",
    "\n",
    "The names of agents are: \n",
    "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "87397826",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space(\"adversary_0\").shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ad6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_counts():\n",
    "    all_agents = 0\n",
    "    adversaries = 0\n",
    "    for agent in env.world.agents:\n",
    "        all_agents += 1\n",
    "        adversaries += 1 if agent.adversary else 0\n",
    "    good_agents = all_agents - adversaries\n",
    "    return (adversaries, good_agents)\n",
    "\n",
    "def process_config(config):\n",
    "    for k, v in config.all.items():\n",
    "        config.adversary[k] = v\n",
    "        config.agent[k] = v\n",
    "\n",
    "n_adversaries, n_good_agents = get_agent_counts()\n",
    "config = AttrDict(\n",
    "    all=AttrDict(\n",
    "        message_size=4,\n",
    "        hidden_size=128,\n",
    "        n_actions=env.action_space(env.agent_selection).n,\n",
    "    )\n",
    "    adversary=AttrDict(\n",
    "        n_agents=n_adversaries,\n",
    "        observation_shape=env.observation_space(\"adversary_0\").shape\n",
    "\n",
    "    ),\n",
    "    agent=AttrDict(\n",
    "        n_agents=n_good_agents,\n",
    "        observation_shape=env.observation_space(\"agent_0\").shape\n",
    "    )\n",
    ")\n",
    "process_config(config)\n",
    "\n",
    "class Container(object):\n",
    "    \"\"\"Container of messages and hidden states of agents in environment.\"\"\"\n",
    "    \n",
    "    def reset(self):\n",
    "        keys = [*self.__message_d.keys()]\n",
    "        for k in keys:\n",
    "            del self.__message_d[k]\n",
    "        keys = [*self.__hidden_d.keys()]\n",
    "         for k in keys:\n",
    "            del self.__hidden_d[k]\n",
    "        self.__message_d[\"adversary\"] = torch.zeros(\n",
    "            self.config.adversary.n_agents*self.config.adversary.message_size,\n",
    "            dtype=torch.float\n",
    "        )\n",
    "        self.__message_d[\"agent\"]     = torch.zeros(\n",
    "            self.config.agent.n_agents*self.config.agent.message_size,\n",
    "            dtype=torch.float\n",
    "        )\n",
    "        for idx in range(self.n_adversaries):\n",
    "            self.__hidden_d[f\"adversary_{idx}\"] = torch.zeros(\n",
    "                self.config.adversary.hidden_size,\n",
    "                dtype=torch.float\n",
    "            )\n",
    "        for idx in range(self.n_agents):\n",
    "            self.__hidden_d[f\"agent_{idx}\"]     = torch.zeros(\n",
    "                self.config.agent.hidden_size,\n",
    "                dtype=torch.float\n",
    "            )\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        self.config\n",
    "        self.__message_d = {}\n",
    "        self.__hidden_d = {}\n",
    "        self.reset()\n",
    "    \n",
    "    def get_message(self, agent_name):\n",
    "        if \"adversary\" in agent_name:\n",
    "            return self.__message_d[\"adversary\"]\n",
    "        elif \"agent\" in agent_name:\n",
    "            return self.__message_d[\"agent\"]\n",
    "        else:\n",
    "            raise ValueError(f\"{agent_name} is neither an agent or adversary.\")\n",
    "    \n",
    "    def update_message(self, agent_name, message):\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        self.__message_d[agent_type][agent_idx:agent_idx + self.__message_size] = message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dce3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple_tag_v2\n",
    "\n",
    "class SimpleTagNet(torch.nn.Module):\n",
    "        \n",
    "    def __init__(self, config, agent_type):\n",
    "        super().__init__()\n",
    "        self.observation_size = config[agent_type].observation_size\n",
    "        self.message_size = config[agent_type].message_size\n",
    "        self.hidden_size = config[agent_type].hidden_size\n",
    "        self.n_agents = config[agent_type].n_agents\n",
    "        self.n_actions = config[agent_type].n_actions\n",
    "        \n",
    "        self.agent_lookup  = torch.nn.Embedding(self.n_agents, self.hidden_size)\n",
    "        self.action_lookup = torch.nn.Embedding(self.n_actions, self.hidden_size)\n",
    "        self.state_mlp     = torch.nn.Linear(self.observation_size, self.hidden_size)\n",
    "        self.message_size  = torch.nn.Linear(self.self.message_size, self.hidden_size)\n",
    "        self.rnn = torch.nn.GRU(128)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3948365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_name = \"adversary_0\"\n",
    "agent_type, agent_idx = agent_name.split(\"_\")\n",
    "int(agent_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4ede7b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 1., 2., 3., 4., 0., 0., 0., 0.]), 4)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(4*3)\n",
    "b = torch.tensor([1,2,3,4])\n",
    "a[4:8] = b\n",
    "a, b.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5b8f3d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 5])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(10)\n",
    "a[range(3,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f86814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06c406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
