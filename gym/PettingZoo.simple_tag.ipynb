{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "50130d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import enum\n",
    "\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.utils import random_demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7724bfe",
   "metadata": {},
   "source": [
    "Arguments in instantiate environment.\n",
    "\n",
    "- num_good: number of good agents\n",
    "- num_adversaries: number of adversaries\n",
    "- num_obstacles: number of obstacles\n",
    "- max_cycles: number of frames (a step for each agent) until game terminates\n",
    "- continuous_actions: Whether agent action spaces are discrete(default) or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9858b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=3,\n",
    "    num_adversaries=3,\n",
    "    num_obstacles=2,\n",
    "    max_cycles=100,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cabc86",
   "metadata": {},
   "source": [
    "Adversaries (red) capture non-adversary (green). The map is a 2D grid and everything is initialized in the region [-1, +1]. There doesn't seem to be position clipping for out of bounds, but non-adversary agent are penalized for out of bounds.\n",
    "Agent's observation is a ndarray vector of concatenated data in the following order:\n",
    "\n",
    "1. current velocity (2,)\n",
    "2. current position (2,)\n",
    "3. relative position (2,) of each landmark\n",
    "4. relative position (2,) of each other agent\n",
    "5. velocity (2,) of each other non-adversary agent\n",
    "\n",
    "Actions:\n",
    "\n",
    "- 0 is NOP\n",
    "- 1 is go left\n",
    "- 2 is go right\n",
    "- 3 is go down\n",
    "- 4 is go up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bc35a8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size (138,)\n",
      "Name of current agent adversary_0\n",
      "Observation space of current agent (24,)\n",
      "Action space of current agent Discrete(5)\n",
      "Sample random action from current agent 0\n",
      "\n",
      "D\n",
      "agent's name is adversary_0\n",
      "agent's position and velocity coordinates [0. 0.] [0.90968928 0.33838488]\n",
      "is agent an adversary? True\n",
      "landmark's name is landmark 0\n",
      "landmark's position coordinates (doesn't move) [-0.53688665 -0.01926829]\n"
     ]
    }
   ],
   "source": [
    "# Print variables of the environment\n",
    "# Documentation:   https://www.pettingzoo.ml/api\n",
    "env.reset()\n",
    "print(\"State size\", env.state_space.shape)\n",
    "print(\"Name of current agent\", env.agent_selection)\n",
    "print(\"Observation space of current agent\", env.observation_space(env.agent_selection).shape)\n",
    "print(\"Action space of current agent\", env.action_space(env.agent_selection))\n",
    "print(\"Sample random action from current agent\", env.action_space(env.agent_selection).sample())\n",
    "print()\n",
    "\n",
    "print(\"D\")\n",
    "\n",
    "# select an agent in the environment world, after using env.unwrapped\n",
    "agent = env.world.agents[0]\n",
    "print(\"agent's name is\", agent.name)\n",
    "print(\"agent's position and velocity coordinates\", agent.state.p_vel, agent.state.p_pos)\n",
    "print(\"is agent an adversary?\", agent.adversary)\n",
    "\n",
    "landmark = env.world.landmarks[0]\n",
    "print(\"landmark's name is\", landmark.name)\n",
    "print(\"landmark's position coordinates (doesn't move)\", landmark.state.p_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8f3522ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward -574.4233956074986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2872.116978037493"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo environment with random policy\n",
    "env.reset()\n",
    "random_demo(env, render=True, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "33232068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(observation, agent):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    agent : str\n",
    "    \"\"\"\n",
    "#     print(observation.shape)\n",
    "#     print(agent)\n",
    "    if \"adversary\" in agent:\n",
    "        # adversary\n",
    "        if agent == \"adversary_0\":\n",
    "            return 4\n",
    "        \n",
    "    if \"agent\" in agent:\n",
    "        # non-adversary\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "for agent in env.agent_iter():\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = policy(observation, agent)\n",
    "        env.step(action)\n",
    "    # time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dce3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
