{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50130d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import enum\n",
    "import math\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import matplotlib.pyplot as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "from pettingzoo.utils import random_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7724bfe",
   "metadata": {},
   "source": [
    "Arguments in instantiate environment.\n",
    "\n",
    "- num_good: number of good agents\n",
    "- num_adversaries: number of adversaries\n",
    "- num_obstacles: number of obstacles\n",
    "- max_cycles: number of frames (a step for each agent) until game terminates\n",
    "- continuous_actions: Whether agent action spaces are discrete(default) or continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9858b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peek into unwrapped environment: __class__ __delattr__ __dict__ __dir__ __doc__ __eq__ __format__ __ge__ __getattribute__ __gt__ __hash__ __init__ __init_subclass__ __le__ __lt__ __module__ __ne__ __new__ __reduce__ __reduce_ex__ __repr__ __setattr__ __sizeof__ __str__ __subclasshook__ __weakref__ _accumulate_rewards _agent_selector _clear_rewards _dones_step_first _execute_world_step _index_map _reset_render _set_action _was_done_step action_space action_spaces agent_iter agents close continuous_actions current_actions last local_ratio max_cycles max_num_agents metadata np_random num_agents observation_space observation_spaces observe possible_agents render reset scenario seed state state_space step steps unwrapped viewer world\n"
     ]
    }
   ],
   "source": [
    "env = simple_tag_v2.env(\n",
    "    num_good=3,\n",
    "    num_adversaries=3,\n",
    "    num_obstacles=2,\n",
    "    max_cycles=300,\n",
    "    continuous_actions=False\n",
    ").unwrapped\n",
    "print(\"Peek into unwrapped environment:\", *dir(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cabc86",
   "metadata": {},
   "source": [
    "### What are the environment parameters?\n",
    "\n",
    "Adversaries (red) capture non-adversary (green). The map is a 2D grid and everything is initialized in the region [-1, +1]. There doesn't seem to be position clipping for out of bounds, but non-adversary agent are penalized for out of bounds.\n",
    "Agent's observation is a ndarray vector of concatenated data in the following order:\n",
    "\n",
    "1. current velocity (2,)\n",
    "2. current position (2,)\n",
    "3. relative position (2,) of each landmark\n",
    "4. relative position (2,) of each other agent\n",
    "5. velocity (2,) of each other non-adversary agent\n",
    "\n",
    "When there are 3 adverseries and 3 non-adversaries, then advarsary observation space is 24 dimensional and non-advarsary observation space is 22 dimensional.\n",
    "\n",
    "The environment is sequential. Agents move one at a time. Agents are either `adversary_*` for adversary or `agent_*` for non-adversary.\n",
    "\n",
    "Actions:\n",
    "\n",
    "- 0 is NOP\n",
    "- 1 is go left\n",
    "- 2 is go right\n",
    "- 3 is go down\n",
    "- 4 is go up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc35a8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State size (138,)\n",
      "Name of current agent adversary_0\n",
      "Observation space of current agent (24,)\n",
      "Action space of current agent Discrete(5)\n",
      "Sample random action from current agent 3\n",
      "The agent names: adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2\n",
      "\n",
      "agent's name is adversary_0\n",
      "agent's position and velocity coordinates [0. 0.] [ 0.90096416 -0.69681156]\n",
      "is agent an adversary? True\n",
      "landmark's name is landmark 0\n",
      "landmark's position coordinates (doesn't move) [ 0.11157408 -0.68886647]\n"
     ]
    }
   ],
   "source": [
    "# Print variables of the environment\n",
    "# Documentation:   https://www.pettingzoo.ml/api\n",
    "env.reset()\n",
    "print(\"State size\", env.state_space.shape)\n",
    "print(\"Name of current agent\", env.agent_selection)\n",
    "print(\"Observation space of current agent\", env.observation_space(env.agent_selection).shape)\n",
    "print(\"Action space of current agent\", env.action_space(env.agent_selection))\n",
    "print(\"Sample random action from current agent\", env.action_space(env.agent_selection).sample())\n",
    "print(\"The agent names:\", *env.agents)\n",
    "print()\n",
    "\n",
    "# select an agent in the environment world, after using env.unwrapped\n",
    "agent = env.world.agents[0]\n",
    "print(\"agent's name is\", agent.name)\n",
    "print(\"agent's position and velocity coordinates\", agent.state.p_vel, agent.state.p_pos)\n",
    "print(\"is agent an adversary?\", agent.adversary)\n",
    "\n",
    "landmark = env.world.landmarks[0]\n",
    "print(\"landmark's name is\", landmark.name)\n",
    "print(\"landmark's position coordinates (doesn't move)\", landmark.state.p_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3522ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward -3420.714186464787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-17103.570932323935"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo environment with random policy\n",
    "env.reset()\n",
    "random_demo(env, render=True, episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33232068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode ran for 605 steps\n"
     ]
    }
   ],
   "source": [
    "def hardcode_policy(observation, agent):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ==========\n",
    "    agent : str\n",
    "    \"\"\"\n",
    "#     print(observation.shape)agent_step_idx\n",
    "#     print(agent)\n",
    "    if \"adversary\" in agent:\n",
    "        # adversary\n",
    "        if agent == \"adversary_0\":\n",
    "            return np.random.binomial(2, 0.3) + 3\n",
    "        \n",
    "    if \"agent\" in agent:\n",
    "        # non-adversary\n",
    "        pass\n",
    "    return 0\n",
    "\n",
    "env.reset()\n",
    "for agent_step_idx, agent in enumerate(env.agent_iter()):\n",
    "    env.render()\n",
    "    observation, reward, done, info = env.last()\n",
    "    if done:\n",
    "        env.step(None)\n",
    "    else:\n",
    "        action = hardcode_policy(observation, agent)\n",
    "        env.step(action)\n",
    "    # time.sleep(0.1)\n",
    "\n",
    "print(f\"episode ran for {agent_step_idx} steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6301c6a",
   "metadata": {},
   "source": [
    "### How to train the agents?\n",
    "\n",
    "- Use the differental inter-agent learning (DIAL) algorithm.\n",
    "- Use parameter sharing for DAIL agents. Separate parameter sets for adversary agents and good agents.\n",
    "- It's not entirely clear the authors accumulate gradients for differentiable communication, but it \n",
    "\n",
    "Messages are vectors. Length 4, 5 should work.\n",
    "\n",
    "Concatenate the messages from all the actors and add them to the message input for the current agent.\n",
    "\n",
    "The names of agents are: \n",
    "adversary_0 adversary_1 adversary_2 agent_0 agent_1 agent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90c62b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5ad6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agent_counts():\n",
    "    all_agents = 0\n",
    "    adversaries = 0\n",
    "    for agent in env.world.agents:\n",
    "        all_agents += 1\n",
    "        adversaries += 1 if agent.adversary else 0\n",
    "    good_agents = all_agents - adversaries\n",
    "    return (adversaries, good_agents)\n",
    "\n",
    "def process_config(config):\n",
    "    for k, v in config.all.items():\n",
    "        config.adversary[k] = v\n",
    "        config.agent[k] = v\n",
    "\n",
    "n_adversaries, n_good_agents = get_agent_counts()\n",
    "config = AttrDict(\n",
    "    discount = 0.99,\n",
    "    epsilon = 0.05,\n",
    "    n_episodes=200,\n",
    "    update_target_interval=10,\n",
    "    report_interval=20,\n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    all=AttrDict(\n",
    "        message_size=4,\n",
    "        hidden_size=128,\n",
    "        n_actions=env.action_space(env.agent_selection).n,\n",
    "        n_rnn_layers=2,\n",
    "        apply_bn=False,\n",
    "    ),\n",
    "    adversary=AttrDict(\n",
    "        n_agents=n_adversaries,\n",
    "        observation_shape=env.observation_space(\"adversary_0\").shape\n",
    "\n",
    "    ),\n",
    "    agent=AttrDict(\n",
    "        n_agents=n_good_agents,\n",
    "        observation_shape=env.observation_space(\"agent_0\").shape\n",
    "    )\n",
    ")\n",
    "process_config(config)\n",
    "\n",
    "class Container(object):\n",
    "    \"\"\"Container of messages and hidden states of agents in environment.\"\"\"\n",
    "    \n",
    "    def reset(self):\n",
    "#         keys = [*self.__message_d.keys()]\n",
    "#         for k in keys:\n",
    "#             del self.__message_d[k]\n",
    "#         keys = [*self.__hidden_d.keys()]\n",
    "#         for k in keys:\n",
    "#             del self.__hidden_d[k]\n",
    "        \n",
    "        for idx in range(config.adversary.n_agents):\n",
    "            self.__message_d[f\"adversary_{idx}\"] = torch.zeros(\n",
    "                self.config.adversary.message_size*(config.adversary.n_agents - 1),\n",
    "                dtype=torch.float\n",
    "            )\n",
    "            self.__hidden_d[f\"adversary_{idx}\"]  = torch.zeros(\n",
    "                (config.adversary.n_rnn_layers, 1, self.config.adversary.hidden_size,),\n",
    "                dtype=torch.float\n",
    "            )\n",
    "        for idx in range(config.agent.n_agents):\n",
    "            self.__message_d[f\"agent_{idx}\"] = torch.zeros(\n",
    "                self.config.agent.message_size*(config.agent.n_agents - 1),\n",
    "                dtype=torch.float\n",
    "            )\n",
    "            self.__hidden_d[f\"agent_{idx}\"]  = torch.zeros(\n",
    "                (config.agent.n_rnn_layers, 1, self.config.agent.hidden_size,),\n",
    "                dtype=torch.float\n",
    "            )\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.__message_d = {}\n",
    "        self.__hidden_d = {}\n",
    "        self.reset()\n",
    "    \n",
    "    def get_message(self, agent_name):\n",
    "        return self.__message_d[agent_name]\n",
    "\n",
    "    def get_hidden(self, agent_name):\n",
    "        return self.__hidden_d[agent_name]\n",
    "\n",
    "    def update_message(self, agent_name, message):\n",
    "        \"\"\"Update message cache.\n",
    "        \n",
    "        Messages of multiple agents are concatenated together.\n",
    "        For example, if agent 2 receives messages from agents 0, 1, and 3 then\n",
    "        the message is a vector of the form: [ 0's message, 1's message, 3's message ]\n",
    "        \"\"\"\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        for jdx in range(config[agent_type].n_agents):\n",
    "            if jdx < agent_idx:\n",
    "                start_idx = agent_idx - 1\n",
    "            elif jdx == agent_idx: # down update message to oneself\n",
    "                continue\n",
    "            else: # agent_idx < jdx\n",
    "                start_idx = agent_idx\n",
    "            end_idx   = start_idx + self.config[agent_type].message_size\n",
    "            # print(jdx, agent_idx, self.__message_d[f\"{agent_type}_{jdx}\"].shape, start_idx, end_idx)\n",
    "            messages = self.__message_d[f\"{agent_type}_{jdx}\"]\n",
    "            self.__message_d[f\"{agent_type}_{jdx}\"] = \\\n",
    "                    torch.hstack((messages[:start_idx], message, messages[end_idx:]))\n",
    "\n",
    "    def update_hidden(self, agent_name, hidden):\n",
    "        self.__hidden_d[agent_name] = hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9dce3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTagNet(torch.nn.Module):\n",
    "    \"\"\"NN Model for the agents. Both good agents and adversaries use this model.\"\"\"\n",
    "        \n",
    "    def __init__(self, config, agent_type):\n",
    "        super().__init__()\n",
    "        # self.config = config\n",
    "        self.observation_size = math.prod(config[agent_type].observation_shape)\n",
    "        self.send_message_size = config[agent_type].message_size\n",
    "        self.recv_message_size = config[agent_type].message_size*(config[agent_type].n_agents - 1)\n",
    "        self.n_agents = config[agent_type].n_agents\n",
    "        self.n_actions = config[agent_type].n_actions\n",
    "        self.apply_bn = config[agent_type].apply_bn\n",
    "        self.hidden_size = config[agent_type].hidden_size\n",
    "        self.n_rnn_layers = config[agent_type].n_rnn_layers\n",
    "        self.n_output = self.n_actions + self.send_message_size\n",
    "        \n",
    "        self.agent_lookup    = torch.nn.Embedding(self.n_agents, self.hidden_size)\n",
    "        self.action_lookup   = torch.nn.Embedding(self.n_actions, self.hidden_size)\n",
    "        self.observation_mlp = torch.nn.Sequential(collections.OrderedDict([\n",
    "            (\"linear\", torch.nn.Linear(self.observation_size, self.hidden_size)),\n",
    "            (\"relu\", torch.nn.ReLU(inplace=True)),\n",
    "        ]))\n",
    "        self.message_mlp = torch.nn.Sequential()\n",
    "        # if self.apply_bn:\n",
    "        #     # input must have shape (N, C), output has the same shape\n",
    "        #     self.message_mlp.add_module(\"bn\", torch.nn.BatchNorm1d(self.recv_message_size))\n",
    "        self.message_mlp.add_module(\"linear\", torch.nn.Linear(self.recv_message_size, self.hidden_size))\n",
    "        self.message_mlp.add_module(\"relu\", torch.nn.ReLU(inplace=True))\n",
    "        \n",
    "        # input must have shape (N, L, H_in)\n",
    "        # output has shape  (N, L, H_out)\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=self.hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.n_rnn_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.output_mlp = torch.nn.Sequential()\n",
    "        self.output_mlp.add_module(\"linear\", torch.nn.Linear(self.hidden_size, self.hidden_size))\n",
    "        self.output_mlp.add_module(\"relu\", torch.nn.ReLU(inplace=True))\n",
    "        self.output_mlp.add_module(\"linear\", torch.nn.Linear(self.hidden_size, self.n_output))\n",
    "    \n",
    "    def forward(self, agent_idx, observation, message, hidden):\n",
    "        \"\"\"Apply DQN to episode step.\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        agent_idx : int\n",
    "            Index of agent\n",
    "        observation : ndarray\n",
    "            The observation vector obtained from the environment.\n",
    "        message : torch.Tensor\n",
    "            Messages from the other agents. By default has shape (message_size*(n_agents - 1))\n",
    "            where message_size=4 and n_agents=3\n",
    "        hidden : torch.Tensor\n",
    "            Hidden state of GRU. By default has shape (n_layers=2, N=1, H_out=128).\n",
    "        \n",
    "        Returns\n",
    "        =======\n",
    "        torch.Tensor\n",
    "            Vector of Q-value associated with each action.\n",
    "        torch.Tensor\n",
    "            The message to pass to other agents.\n",
    "        torch.Tensor\n",
    "            The hidden state used by GRU.\n",
    "        \"\"\"\n",
    "        agent_idx   = torch.tensor(agent_idx, dtype=torch.int)\n",
    "        observation = torch.tensor(observation, dtype=torch.float)\n",
    "        z_a = self.agent_lookup(agent_idx)\n",
    "        z_o = self.observation_mlp(observation)\n",
    "        z_m = self.message_mlp(message)\n",
    "        z = z_a + z_o + z_m\n",
    "        # z has shape (N=1, L=1, H_in=128)\n",
    "        z = z.unsqueeze(0).unsqueeze(0)\n",
    "        # hidden has shape (n_layers=2, N=1, H_out=128) before and after\n",
    "        # out has shape (N=1, L=1, H_out=128)\n",
    "        out, hidden = self.rnn(z, hidden)\n",
    "        out = out.squeeze(0).squeeze(0)\n",
    "        out = self.output_mlp(out)\n",
    "        Q = out[0:self.n_actions]\n",
    "        m = out[self.n_actions:self.n_actions + self.send_message_size]\n",
    "        return Q, m, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f86814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on episode 20\n",
      "     loss: adversary 600.0, agent 245.99307250976562\n",
      "     reward: adversary 60.0, agent -63.05876532367346\n",
      "on episode 40\n",
      "     loss: adversary 0.0, agent 1843.7525634765625\n",
      "     reward: adversary 0.0, agent -914.5711822726826\n",
      "on episode 60\n",
      "     loss: adversary 1200.0, agent 401.1001892089844\n",
      "     reward: adversary 120.0, agent -44.19560140241495\n",
      "on episode 80\n",
      "     loss: adversary 0.0, agent 418.18951416015625\n",
      "     reward: adversary 0.0, agent -161.47785737370188\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27998/1040620320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0madversary_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0madversary_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_27998/1040620320.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepisode_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         episode = run_episode(config, container, adversary_net, agent_net,\n\u001b[0m\u001b[1;32m    148\u001b[0m                               should_render=episode_idx % config.report_interval == 0 and episode_idx > 0)\n\u001b[1;32m    149\u001b[0m         train_agents(config, episode, adversary_net, agent_net, adversary_target_net, agent_target_net,\n",
      "\u001b[0;32m/tmp/ipykernel_27998/1040620320.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(config, container, adversary_net, agent_net, should_render)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mQ_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madversary_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# good agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mQ_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_curr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_curr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/miniconda3/envs/ml8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_27998/2505600660.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, agent_idx, observation, message, hidden)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/miniconda3/envs/ml8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/miniconda3/envs/ml8/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/miniconda3/envs/ml8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/miniconda3/envs/ml8/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/miniconda3/envs/ml8/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def choose_action(config, agent_type, Q):\n",
    "    if random.random() < config.epsilon:\n",
    "        return random.randrange(config[agent_type].n_actions)\n",
    "    else:\n",
    "        return torch.argmax(Q).item()\n",
    "\n",
    "def run_episode(config, container, adversary_net, agent_net, should_render=False):\n",
    "    \"\"\"\n",
    "    inputs consist of observation, message (backprop), hidden (backprop) indexed by agent\n",
    "    outputs consist of action, q-value of action (backprop), reward, done indexed by (step, agent)\n",
    "    \"\"\"\n",
    "    episode = AttrDict(steps=0, reward=AttrDict(adversary=0, agent=0), step_records=[])\n",
    "    n_agents = config.adversary.n_agents + config.agent.n_agents\n",
    "    step_record = None\n",
    "    \n",
    "    env.reset()\n",
    "    for agent_step_idx, agent_name in enumerate(env.agent_iter()):\n",
    "        if should_render:\n",
    "            env.render()\n",
    "        if agent_step_idx % n_agents == 0:\n",
    "            episode.steps += 1\n",
    "            step_record = AttrDict(adversary={}, agent={})\n",
    "            episode.step_records.append(step_record)\n",
    "            \n",
    "        obs_curr, reward, done, _ = env.last()\n",
    "        agent_type, agent_idx = agent_name.split(\"_\")\n",
    "        agent_idx = int(agent_idx)\n",
    "        if done:\n",
    "            step_record[agent_type][agent_idx] = AttrDict(\n",
    "                observation=obs_curr,\n",
    "                message=None,\n",
    "                hidden=None,\n",
    "                action=None,\n",
    "                Q=None,\n",
    "                reward=reward,\n",
    "                done=done,\n",
    "            )\n",
    "            env.step(None)\n",
    "            continue\n",
    "            \n",
    "        m_prev = container.get_message(agent_name)\n",
    "        h_prev = container.get_hidden(agent_name)\n",
    "        if agent_type == \"adversary\":\n",
    "            Q_curr, m_curr, h_curr = adversary_net(agent_idx, obs_curr, m_prev, h_prev)\n",
    "        else: # good agent\n",
    "            Q_curr, m_curr, h_curr = agent_net(agent_idx, obs_curr, m_prev, h_prev)\n",
    "\n",
    "        action = choose_action(config, agent_type, Q_curr)\n",
    "        env.step(action)\n",
    "        container.update_message(agent_name, m_curr)\n",
    "        container.update_hidden(agent_name, h_curr)\n",
    "        step_record[agent_type][agent_idx] = AttrDict(\n",
    "            # inputs to network\n",
    "            observation=obs_curr,\n",
    "            message=m_prev,\n",
    "            hidden=h_prev,\n",
    "            # outputs of network / inputs to environment\n",
    "            action=action,\n",
    "            Q=Q_curr,\n",
    "            # output of environment\n",
    "            reward=reward,\n",
    "            done=done,\n",
    "        )\n",
    "        episode.reward[agent_type] += reward\n",
    "    \n",
    "    return episode\n",
    "\n",
    "def train_agents(config, episode, adversary_net, agent_net,\n",
    "                 adversary_target_net, agent_target_net,\n",
    "                 adversary_optimizer, agent_optimizer):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    discount = torch.tensor(config.discount, dtype=torch.float)\n",
    "    \n",
    "    adversary_loss = torch.tensor(0.)\n",
    "    agent_loss = torch.tensor(0.)\n",
    "    for step_idx in range(episode.steps):\n",
    "        \n",
    "        for agent_idx in episode.step_records[step_idx].adversary.keys():\n",
    "            curr_record = episode.step_records[step_idx].adversary[agent_idx]\n",
    "            if curr_record.done:\n",
    "                # agent is done at this step\n",
    "                continue\n",
    "            next_record = episode.step_records[step_idx + 1].adversary[agent_idx]\n",
    "            r = torch.tensor(next_record.reward, dtype=torch.float)\n",
    "            y = None\n",
    "            if next_record.done:\n",
    "                # agent terminates at next step\n",
    "                y = r\n",
    "            else:\n",
    "                next_o = next_record.observation\n",
    "                next_m = next_record.message\n",
    "                next_h = next_record.hidden\n",
    "                target_Q, _, _ = adversary_target_net(agent_idx, next_o, next_m, next_h)\n",
    "                max_target_Q = torch.max(target_Q)\n",
    "                y = r + discount*max_target_Q\n",
    "            u = curr_record.action\n",
    "            Q_u = curr_record.Q[u]\n",
    "            adversary_loss += torch.pow(y - Q_u, 2.)\n",
    "            \n",
    "        for agent_idx in episode.step_records[step_idx].agent.keys():\n",
    "            curr_record = episode.step_records[step_idx].agent[agent_idx]\n",
    "            if curr_record.done:\n",
    "                # agent is done at this step\n",
    "                continue\n",
    "            next_record = episode.step_records[step_idx + 1].agent[agent_idx]\n",
    "            r = torch.tensor(next_record.reward, dtype=torch.float)\n",
    "            y = None\n",
    "            if next_record.done:\n",
    "                # agent terminates at next step\n",
    "                y = r\n",
    "            else:\n",
    "                next_o = next_record.observation\n",
    "                next_m = next_record.message\n",
    "                next_h = next_record.hidden\n",
    "                target_Q, _, _ = agent_target_net(agent_idx, next_o, next_m, next_h)\n",
    "                max_target_Q = torch.max(target_Q)\n",
    "                y = r + discount*max_target_Q\n",
    "            u = curr_record.action\n",
    "            Q_u = curr_record.Q[u]\n",
    "            agent_loss += torch.pow(y - Q_u, 2.)\n",
    "    \n",
    "    adversary_optimizer.zero_grad()\n",
    "    agent_optimizer.zero_grad()\n",
    "    adversary_loss.backward()\n",
    "    agent_loss.backward()\n",
    "    adversary_optimizer.step()\n",
    "    agent_optimizer.step()\n",
    "    episode.loss = AttrDict(adversary=adversary_loss.item(), agent=agent_loss.item())\n",
    "    \n",
    "\n",
    "def train(config):\n",
    "    \"\"\"\n",
    "    - Use parameter sharing between agents of the same class.\n",
    "    - Good agents use one RL model, adversaries use another RL model.\n",
    "      Train the agents side by side.\n",
    "    - Separate, disjoint communication channels for two classes of agents,\n",
    "      maintained by a container to store the messages.\n",
    "    \"\"\"\n",
    "    adversary_net = SimpleTagNet(config, \"adversary\")\n",
    "    agent_net = SimpleTagNet(config, \"agent\")\n",
    "    adversary_target_net = SimpleTagNet(config, \"adversary\")\n",
    "    agent_target_net = SimpleTagNet(config, \"agent\")\n",
    "    adversary_target_net.eval()\n",
    "    agent_target_net.eval()\n",
    "    adversary_optimizer = torch.optim.RMSprop(adversary_net.parameters())\n",
    "    agent_optimizer = torch.optim.RMSprop(agent_net.parameters())\n",
    "    container = Container(config)\n",
    "    def update_targets():\n",
    "        adversary_target_net.load_state_dict(adversary_net.state_dict())\n",
    "        agent_target_net.load_state_dict(agent_net.state_dict())\n",
    "    \n",
    "    for episode_idx in range(config.n_episodes):\n",
    "        episode = run_episode(config, container, adversary_net, agent_net,\n",
    "                              should_render=episode_idx % config.report_interval == 0 and episode_idx > 0)\n",
    "        train_agents(config, episode, adversary_net, agent_net, adversary_target_net, agent_target_net,\n",
    "                     adversary_optimizer, agent_optimizer)\n",
    "\n",
    "        if episode_idx % config.update_target_interval == 0 and episode_idx > 0:\n",
    "            update_targets()\n",
    "        if episode_idx % config.report_interval == 0 and episode_idx > 0:\n",
    "            print(f\"on episode {episode_idx}\")\n",
    "            print(f\"     loss: adversary {episode.loss.adversary}, agent {episode.loss.agent}\")\n",
    "            print(f\"     reward: adversary {episode.reward.adversary}, agent {episode.reward.agent}\")\n",
    "        container.reset()\n",
    "    \n",
    "    return adversary_net, agent_net\n",
    "\n",
    "adversary_net, agent_net = train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0210fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0af6f10",
   "metadata": {},
   "source": [
    "## Scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c06c406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, tensor(3), tensor(2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1,3,2,0])\n",
    "torch.argmax(a).item(), torch.max(a), a[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33d9b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 "
     ]
    }
   ],
   "source": [
    "d = {1: 'a', 2: 'b', 3: 'c'}\n",
    "for i in d:\n",
    "    print(i , end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "779f4097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6, device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "a = torch.tensor(2, device=device)\n",
    "b = torch.tensor(3)\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53211f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 9, 8, 5])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.arange(6)\n",
    "a = torch.tensor([9, 8])\n",
    "\n",
    "idx = 3\n",
    "\n",
    "torch.hstack((v[:idx], a, v[idx + 2:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701179fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
