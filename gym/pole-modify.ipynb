{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed890253",
   "metadata": {},
   "source": [
    "Pole balancing\n",
    "\n",
    "Based on:\n",
    "<https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09921700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numbers\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import utility as util\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4b84dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=T.InterpolationMode.BICUBIC),\n",
    "                    T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02fa7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = util.AttrDict(\n",
    "        episodes=1_000,\n",
    "        minibatch_size=128,\n",
    "        discount=0.999,\n",
    "        eps_start=0.9,\n",
    "        eps_end=0.05,\n",
    "        eps_decay=200,\n",
    "        update_target_interval=10,\n",
    "        replay_memory_minlen=200,\n",
    "        replay_memory_maxlen=10_000,\n",
    "        aggregate_stats_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9180d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcdfc74f690>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = lambda steps : config.eps_end + (config.eps_start - config.eps_end) * np.exp(-1. * steps / config.eps_decay)\n",
    "n = np.arange(1000)\n",
    "plt.plot(n, f(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e9ad01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transitions and Replay Memory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('curr_state', 'action', 'next_state', 'reward', 'is_done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163a9b5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment attributes: gravity masscart masspole total_mass length polemass_length force_mag tau kinematics_integrator theta_threshold_radians x_threshold action_space observation_space np_random viewer state steps_beyond_done spec\n",
      "Gravitational constant: 9.8\n",
      "Number of possible actions an agent can make: 2\n",
      "Sampling of discrete action space gives: 0\n",
      "Current state is: None\n",
      "States are bounded below by: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "States are bounded above by: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "# Some attributes of Gym environment\n",
    "# Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "print(\"Environment attributes:\", *vars(env).keys())\n",
    "print(\"Gravitational constant:\", env.gravity)\n",
    "print(\"Number of possible actions an agent can make:\", env.action_space.n)\n",
    "print(\"Sampling of discrete action space gives:\", env.action_space.sample())\n",
    "# The (observation) state is an array\n",
    "# [Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity]\n",
    "print(\"Current state is:\", env.state)\n",
    "print(\"States are bounded below by:\", env.observation_space.low)\n",
    "print(\"States are bounded above by:\",env.observation_space.high)\n",
    "# help(env.observation_space)\n",
    "# help(env) # for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8638abe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faaa0849f90>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Getting the first snapshot of the environment after resetting the Gym \n",
    "env.reset()\n",
    "screen = env.render(mode='rgb_array')\n",
    "plt.imshow(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7ec41fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_cart_location(env, screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def preprocess_screen(env, screen):\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = screen.transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(env, screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "def render(env):\n",
    "    return env.render(mode='rgb_array')\n",
    "\n",
    "env.reset()\n",
    "screen = render(env)\n",
    "screen = preprocess_screen(env, screen)\n",
    "plt.imshow(screen.cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97caa60a",
   "metadata": {},
   "source": [
    "## Using CartPole from PyTorch Tutorial\n",
    "\n",
    "Based on:\n",
    "<https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1aaeb1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNet(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Number of Linear input connections depends on output of conv2d layers\n",
    "        # and therefore the input image size, so compute it.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x, is_training=True):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1111ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageStateDQNAgent(object):\n",
    "    \n",
    "    def __update_target_model(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.terminal_state_counter = 0\n",
    "\n",
    "    def __init__(self, env, config, device):\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "        self.discount = self.config.discount\n",
    "        self.eps_end = self.config.eps_end\n",
    "        self.eps_start = self.config.eps_start\n",
    "        self.eps_decay = self.config.eps_decay\n",
    "        self.replay_memory_minlen = self.config.replay_memory_minlen\n",
    "        self.replay_memory_maxlen = self.config.replay_memory_maxlen\n",
    "        self.update_target_interval = self.config.update_target_interval\n",
    "        self.minibatch_size = self.config.minibatch_size\n",
    "        self.device = device\n",
    "        self.env.reset()\n",
    "        init_screen = self.render(is_input=True)\n",
    "        _, _, self.screen_height, self.screen_width = init_screen.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.policy_net = DQNet(self.screen_height, self.screen_width, self.n_actions).to(self.device)\n",
    "        self.target_net = DQNet(self.screen_height, self.screen_width, self.n_actions).to(self.device)\n",
    "        self.target_net.eval()\n",
    "        self.__update_target_model()\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "        self.replay_memory = ReplayMemory(self.replay_memory_maxlen)\n",
    "        self.steps = 0\n",
    "\n",
    "    def update_replay_memory(self, *args):\n",
    "        \"\"\"\n",
    "        Adds step's data to a memory replay array\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        transition : tuple\n",
    "            Contains (curr_state, action, next_state, reward, is_done) where\n",
    "            - curr_state : or s_t, an RGB image of dimensions (channels, height, width)\n",
    "            - action     : or a_t, an integer 0-9.\n",
    "            - next_state : or s_{t+1}, an RGB image of dimensions (channels, height, width)\n",
    "            - reward     : or r_t, a number\n",
    "            - is_done    : a boolean specifying whether s_t is a terminal state\n",
    "                           where the simulation ended.\n",
    "        \"\"\"\n",
    "        self.replay_memory.push(*args)\n",
    "\n",
    "    @property\n",
    "    def epsilon(self):\n",
    "        return self.eps_end + (self.eps_start - self.eps_end) *\\\n",
    "            math.exp(-1. * self.steps / self.eps_decay)\n",
    "        \n",
    "    def select_action(self, state, is_training=True):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.epsilon\n",
    "        if is_training:\n",
    "            self.steps += 1\n",
    "        if is_training and sample <= eps_threshold:\n",
    "            return torch.tensor([[random.randrange(self.n_actions)]], device=self.device, dtype=torch.long)\n",
    "        else:\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad():\n",
    "                values, indices = self.policy_net(state).max(1)\n",
    "            self.policy_net.train()\n",
    "            return indices.view(1, 1)\n",
    "\n",
    "    def __get_cart_location(self, screen_width):\n",
    "        world_width = self.env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(self.env.state[0] * scale + screen_width / 2.0)\n",
    "\n",
    "    def __preprocess_screen(self, screen):\n",
    "        # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "        # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "        screen = screen.transpose((2, 0, 1))\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "        view_width = int(screen_width * 0.6)\n",
    "        cart_location = self.__get_cart_location(screen_width)\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescale, convert to torch tensor\n",
    "        # (this doesn't require a copy)\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        return resize(screen).unsqueeze(0)\n",
    "    \n",
    "    def render(self, is_input=False):\n",
    "        screen = self.env.render(mode='rgb_array')\n",
    "        if is_input:\n",
    "            return self.__preprocess_screen(screen)\n",
    "        else:\n",
    "            return screen\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "    \n",
    "    def train(self, is_terminal_state=False):\n",
    "        if len(self.replay_memory) < self.replay_memory_minlen:\n",
    "            # Start training only if replay memory reached a minimum size\n",
    "            return None\n",
    "        # Get minibatch\n",
    "        transitions = self.replay_memory.sample(self.minibatch_size)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # Compute the current Q values\n",
    "        curr_states = torch.cat(batch.curr_state)\n",
    "        curr_Qs = self.policy_net(curr_states)\n",
    "        # Extract the Q values corresponding to agent actions\n",
    "        actions = torch.cat(batch.action)\n",
    "        curr_Qs = curr_Qs.gather(1, actions).squeeze(1)\n",
    "        # Get rewards, and is_done flags.\n",
    "        reward = torch.cat(batch.reward)\n",
    "        is_done = torch.cat(batch.is_done)\n",
    "        # Get the next states\n",
    "        next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        next_states = next_states\n",
    "        # Compute the max of next Q values\n",
    "        with torch.no_grad():\n",
    "            next_Qs = self.target_net(next_states, is_training=False)\n",
    "        max_next_Qs, _ = next_Qs.max(dim=1)\n",
    "        # Compute the expected Q values for TD error\n",
    "        expected_Qs = torch.zeros(self.minibatch_size,\n",
    "                dtype=torch.float, device=device)\n",
    "        expected_Qs[~is_done] = reward[~is_done] + self.discount*max_next_Qs\n",
    "        expected_Qs[is_done] = reward[is_done]\n",
    "        # Model training step\n",
    "        self.optimizer.zero_grad()\n",
    "        # print(\"curr_Qs\", curr_Qs); print(\"expected_Qs\", expected_Qs)\n",
    "        loss = self.criterion(curr_Qs, expected_Qs)\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        # Update target model if necessary\n",
    "        if is_terminal_state:\n",
    "            self.terminal_state_counter += 1\n",
    "        if self.terminal_state_counter > self.update_target_interval:\n",
    "            self.__update_target_model()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa46bde",
   "metadata": {},
   "source": [
    "Testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c97a3945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0476, 0.0213]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "agent = ImageStateDQNAgent(env, config, device)\n",
    "env.step(1)\n",
    "last_screen = agent.render(is_input=True)\n",
    "env.step(1)\n",
    "curr_screen = agent.render(is_input=True)\n",
    "curr_state = curr_screen - last_screen\n",
    "action = agent.select_action(curr_state, is_training=False)\n",
    "_, reward, is_done, _ = env.step(action.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33359acd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on episode 1; epsilon 0.8307354417412388\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 8.5\n",
      "    min reward 0\n",
      "    max reward 17.0\n",
      "on episode 50; epsilon 0.06556829305542405\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 16.0\n",
      "    min reward 9.0\n",
      "    max reward 38.0\n",
      "on episode 100; epsilon 0.050059322676056554\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 22.28\n",
      "    min reward 8.0\n",
      "    max reward 52.0\n",
      "on episode 150; epsilon 0.050000000408912906\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 47.54\n",
      "    min reward 11.0\n",
      "    max reward 176.0\n",
      "on episode 200; epsilon 0.05000000000007797\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 34.26\n",
      "    min reward 8.0\n",
      "    max reward 140.0\n",
      "on episode 250; epsilon 0.05000000000000004\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 30.74\n",
      "    min reward 8.0\n",
      "    max reward 93.0\n",
      "on episode 300; epsilon 0.05\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 40.66\n",
      "    min reward 10.0\n",
      "    max reward 137.0\n",
      "on episode 350; epsilon 0.05\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 49.32\n",
      "    min reward 11.0\n",
      "    max reward 114.0\n",
      "on episode 400; epsilon 0.05\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 77.1\n",
      "    min reward 12.0\n",
      "    max reward 233.0\n",
      "on episode 450; epsilon 0.05\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 63.16\n",
      "    min reward 10.0\n",
      "    max reward 252.0\n",
      "on episode 500; epsilon 0.05\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 45.38\n",
      "    min reward 10.0\n",
      "    max reward 178.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9197/1266165695.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mshould_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshould_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimagedifference_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/pole-dqnet.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9197/1266165695.py\u001b[0m in \u001b[0;36mimagedifference_train\u001b[0;34m(env, config, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     torch.tensor([is_done],dtype=torch.bool, device=device))\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_terminal_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mepisode_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_9197/4220819502.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, is_terminal_state)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# Get rewards, and is_done flags.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mis_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Get the next states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def imagedifference_train(env, config, device):\n",
    "    agent = ImageStateDQNAgent(env, config, device)\n",
    "    # Reward is 1 for every step taken, including the termination step\n",
    "    episode_rewards = [0]\n",
    "    avgs_reward = []\n",
    "    mins_reward = []\n",
    "    maxs_reward = []\n",
    "    for episode in range(1, config.episodes + 1):\n",
    "        # agent.writer.add_scalar('epsilon', agent.epsilon, episode)\n",
    "        env.reset()\n",
    "        is_done = False\n",
    "        episode_reward = 0\n",
    "        episode_losses = []\n",
    "        last_screen = agent.render(is_input=True)\n",
    "        curr_screen = agent.render(is_input=True)\n",
    "        curr_state = curr_screen - last_screen\n",
    "        while not is_done:\n",
    "            action = agent.select_action(curr_state)\n",
    "            _, reward, is_done, _ = env.step(action.item())\n",
    "            episode_reward += reward\n",
    "            last_screen = curr_screen\n",
    "            curr_screen = agent.render(is_input=True)\n",
    "            if is_done:\n",
    "                next_state = None\n",
    "            else:\n",
    "                next_state = curr_screen - last_screen\n",
    "            agent.update_replay_memory(\n",
    "                    curr_state,\n",
    "                    action,\n",
    "                    next_state,\n",
    "                    torch.tensor([reward], device=device),\n",
    "                    torch.tensor([is_done],dtype=torch.bool, device=device))\n",
    "            loss = agent.train(is_terminal_state=is_done)\n",
    "            if isinstance(loss, numbers.Number):\n",
    "                episode_losses.append(loss)\n",
    "            curr_state = next_state\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        avg_losses = sum(episode_losses) / len(episode_losses) if len(episode_losses) > 0 else 0\n",
    "        # agent.writer.add_scalar('avg_loss', avg_losses, episode)\n",
    "        if episode % config.aggregate_stats_interval == 0 or episode == 1:\n",
    "            episode_rewards = episode_rewards[-config.aggregate_stats_interval:]\n",
    "            avg_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "            min_reward = min(episode_rewards)\n",
    "            max_reward = max(episode_rewards)\n",
    "            # agent.writer.add_scalar('avg_reward', avg_reward, episode)\n",
    "            # agent.writer.add_scalar('min_reward', min_reward, episode)\n",
    "            # agent.writer.add_scalar('max_reward', max_reward, episode)\n",
    "            avgs_reward.append(avg_reward)\n",
    "            mins_reward.append(min_reward)\n",
    "            maxs_reward.append(max_reward)\n",
    "            print(f\"on episode {episode}; epsilon {agent.epsilon}\")\n",
    "            print(f\"    stats aggregated over the last {config.aggregate_stats_interval} episodes:\")\n",
    "            print(f\"    avg reward {avg_reward}\")\n",
    "            print(f\"    min reward {min_reward}\")\n",
    "            print(f\"    max reward {max_reward}\")\n",
    "            \n",
    "    plt.plot(np.arange(len(avgs_reward)), avgs_reward, label='avg reward')\n",
    "    plt.plot(np.arange(len(mins_reward)), mins_reward, label='min reward')\n",
    "    plt.plot(np.arange(len(maxs_reward)), maxs_reward, label='max reward')\n",
    "    plt.legend()\n",
    "    plt.ylabel(f\"rewards aggregated over the last {config.aggregate_stats_interval} episodes\")\n",
    "    plt.xlabel(f\"episode #\")\n",
    "    # plt.savefig(f\"{agent.log_dir}/training-summary.png\")\n",
    "    return agent\n",
    "\n",
    "\n",
    "should_train = True\n",
    "if should_train:\n",
    "    agent = imagedifference_train(env, config, device)\n",
    "    torch.save(agent.policy_net.state_dict(), 'models/pole-dqnet.pth')\n",
    "    net = agent.policy_net\n",
    "    net.eval()\n",
    "    plt.show()\n",
    "else:\n",
    "    agent = ImageStateDQNAgent(env, config, device)\n",
    "    agent.policy_net.load_state_dict(torch.load('models/pole-dqnet.pth'))\n",
    "    net = agent.policy_net\n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "080fecc3",
   "metadata": {},
   "source": [
    "on episode 1; epsilon 0.8307354417412388\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 8.5\n",
    "    min reward 0\n",
    "    max reward 17.0\n",
    "on episode 50; epsilon 0.06556829305542405\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 16.0\n",
    "    min reward 9.0\n",
    "    max reward 38.0\n",
    "on episode 100; epsilon 0.050059322676056554\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 22.28\n",
    "    min reward 8.0\n",
    "    max reward 52.0\n",
    "on episode 150; epsilon 0.050000000408912906\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 47.54\n",
    "    min reward 11.0\n",
    "    max reward 176.0\n",
    "on episode 200; epsilon 0.05000000000007797\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 34.26\n",
    "    min reward 8.0\n",
    "    max reward 140.0\n",
    "on episode 250; epsilon 0.05000000000000004\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 30.74\n",
    "    min reward 8.0\n",
    "    max reward 93.0\n",
    "on episode 300; epsilon 0.05\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 40.66\n",
    "    min reward 10.0\n",
    "    max reward 137.0\n",
    "on episode 350; epsilon 0.05\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 49.32\n",
    "    min reward 11.0\n",
    "    max reward 114.0\n",
    "on episode 400; epsilon 0.05\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 77.1\n",
    "    min reward 12.0\n",
    "    max reward 233.0\n",
    "on episode 450; epsilon 0.05\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 63.16\n",
    "    min reward 10.0\n",
    "    max reward 252.0\n",
    "on episode 500; epsilon 0.05\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 45.38\n",
    "    min reward 10.0\n",
    "    max reward 178.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecdc6a",
   "metadata": {},
   "source": [
    "Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcfb4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't work?\n",
    "agent.reset()\n",
    "last_screen = agent.render(is_input=True)\n",
    "is_done = False\n",
    "while not is_done:\n",
    "    curr_screen = agent.render(is_input=True)\n",
    "    curr_state = curr_screen - last_screen\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        values, indices = net(curr_state).max(1)\n",
    "        action = indices.view(1, 1)\n",
    "\n",
    "    _, _, is_done, _ = env.step(action.item())\n",
    "    last_screen = curr_screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acdbfac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d33f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
