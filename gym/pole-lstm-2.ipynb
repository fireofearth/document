{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8acd19f8",
   "metadata": {},
   "source": [
    "Based on:  \n",
    "<https://marl-ieee-nitk.github.io/deep-reinforcement-learning/2019/01/06/DRQN.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8edf6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8b3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model,self).__init__()\n",
    "        self.hidden_size = 512\n",
    "        self.conv1=nn.Conv2d(4,32,kernel_size=8,stride=4)\n",
    "        self.bn1=nn.BatchNorm2d(32)\n",
    "        self.conv2=nn.Conv2d(32,64,kernel_size=4,stride =2)\n",
    "        self.bn2=nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.rnn= nn.RNN(input_size=64*7*7, hidden_size=512,num_layers=2,batch_first=True)\n",
    "        self.fc = nn.Linear(512, 2)\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        return (torch.zeros(2,batch_size, self.hidden_size))\n",
    "    \n",
    "    def forward(self,x,hidden):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x=x.reshape(x.shape[0],1,7*7*64)\n",
    "        x,h_0=self.rnn(x,hidden)\n",
    "        return self.fc(x.contiguous().view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "496db4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00305369, -0.01995676, -0.03413646,  0.04732984])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0').unwrapped\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b64abfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (rnn): RNN(3136, 512, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy=model()\n",
    "target_net=model()\n",
    "target_net.load_state_dict(policy.state_dict())\n",
    "target_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e3e1540",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(policy.parameters())\n",
    "criterion = F.smooth_l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ef2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory=10000\n",
    "store=[[dict()] for i in range(memory)]\n",
    "gamma=0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78bd3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PIL2array(img):\n",
    "    return np.array(img.getdata(),np.uint8).reshape(img.size[1], img.size[0], 4)\n",
    "\n",
    "def array2PIL(arr, size):\n",
    "    mode = 'RGBA'\n",
    "    arr = arr.reshape(arr.shape[0]*arr.shape[1], arr.shape[2])\n",
    "    if len(arr[0]) == 3:\n",
    "        arr = np.c_[arr, 255*np.ones((len(arr),1), np.uint8)]\n",
    "    return Image.frombuffer(mode, size, arr.tostring(), 'raw', mode, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7340824e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processScreen(screen):\n",
    "    s=[600,400]\n",
    "    image= array2PIL(screen,s)\n",
    "    newImage = image.resize((84, 84))\n",
    "    xtt=PIL2array(newImage)\n",
    "    xtt=xtt.reshape(xtt.shape[2],xtt.shape[0],xtt.shape[1])\n",
    "    img=torch.from_numpy(np.array(xtt))\n",
    "    img=img.type('torch.FloatTensor')\n",
    "    return img/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "859db51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEpisode(ind,prev,curr,reward,act):\n",
    "    if len(store[ind]) ==0:\n",
    "        store[ind][0]={'prev':prev,'curr':curr,'reward':reward,'action':act}\n",
    "    else:\n",
    "        store[ind].append({'prev':prev,'curr':curr,'reward':reward,'action':act})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "357c18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNet(total_episodes):\n",
    "    if total_episodes==0:\n",
    "        return\n",
    "    ep=random.randint(0,total_episodes-1)\n",
    "    if len(store[ep]) < 8:\n",
    "        return\n",
    "    else:  \n",
    "        start=random.randint(1,len(store[ep])-1)\n",
    "        length=len(store[ep])\n",
    "        inp=[]\n",
    "        target=[]\n",
    "        rew=torch.Tensor(1,length-start)\n",
    "        actions=torch.Tensor(1,length-start)\n",
    "        \n",
    "        for i in range(start,length,1):\n",
    "            inp.append((store[ep][i]).get('prev'))\n",
    "            target.append((store[ep][i]).get('curr'))\n",
    "            rew[0][i-start]=store[ep][i].get('reward')\n",
    "            actions[0][i-start]=store[ep][i].get('action')\n",
    "        targets = torch.Tensor(target[0].shape[0],target[0].shape[1],target[0].shape[2])\n",
    "        torch.cat(target, out=targets)\n",
    "        ccs=torch.Tensor(inp[0].shape[0],inp[0].shape[1],inp[0].shape[2])\n",
    "        torch.cat(inp, out=ccs)\n",
    "        hidden = policy.init_hidden(length-start)\n",
    "        qvals= target_net(targets,hidden)\n",
    "        actions=actions.type('torch.LongTensor')\n",
    "        actions=actions.reshape(length-start,1)\n",
    "        hidden = policy.init_hidden(length-start)\n",
    "        inps=policy(ccs,hidden).gather(1,actions)\n",
    "        p1,p2=qvals.detach().max(1)\n",
    "        targ = torch.Tensor(1,p1.shape[0])   \n",
    "        for num in range(start,length,1):\n",
    "            if num==len(store[ep])-1:\n",
    "                targ[0][num-start]=rew[0][num-start] \n",
    "            else:\n",
    "                targ[0][num-start]=rew[0][num-start]+gamma*p1[num-start]\n",
    "        optimizer.zero_grad()\n",
    "        inps=inps.reshape(1,length-start)\n",
    "        loss = criterion(inps,targ)\n",
    "        loss.backward()\n",
    "        for param in policy.parameters():\n",
    "            param.grad.data.clamp(-1,1)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c5949d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDRQN(episodes):\n",
    "    steps_done=0\n",
    "    episode_rewards = [0]\n",
    "    for i in range(0,episodes,1):        \n",
    "        # print(\"Episode\",i)\n",
    "        env.reset()\n",
    "        prev=env.render(mode='rgb_array')\n",
    "        prev=processScreen(prev)\n",
    "        done=False\n",
    "        steps=0\n",
    "        rew=0\n",
    "        while done == False:\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "            math.exp(-1. * steps_done / EPS_DECAY)\n",
    "            # print(steps,end=\" \")\n",
    "            steps+=1\n",
    "            hidden = policy.init_hidden(1)\n",
    "            output=policy(prev.unsqueeze(0),hidden)\n",
    "            action=(output.argmax()).item()\n",
    "            rand= random.uniform(0,1)\n",
    "            if rand < 0.05:\n",
    "                action=random.randint(0,1)\n",
    "\n",
    "            _,reward,done,_=env.step(action)   \n",
    "            rew=rew+reward\n",
    "            if steps>200:\n",
    "                terminal = torch.zeros(prev.shape[0],prev.shape[1],prev.shape[2])\n",
    "                addEpisode(i,prev.unsqueeze(0),terminal.unsqueeze(0),-10,action)\n",
    "                f=0\n",
    "                break\n",
    "            sc=env.render(mode='rgb_array')\n",
    "            sc=processScreen(sc)\n",
    "            addEpisode(i,prev.unsqueeze(0),sc.unsqueeze(0),reward,action)\n",
    "            trainNet(i)\n",
    "            prev=sc\n",
    "            steps_done+=1\n",
    "        terminal = torch.zeros(prev.shape[0],prev.shape[1],prev.shape[2])\n",
    "        # print(rew)\n",
    "        episode_rewards.append(rew)\n",
    "        if i % 50 == 0:\n",
    "            episode_rewards = episode_rewards[-50:]\n",
    "            avg_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "            min_reward = min(episode_rewards)\n",
    "            max_reward = max(episode_rewards)\n",
    "            print(f\"on episode {i}\")\n",
    "            print(\"    stats aggregated over the last 50 episodes:\")\n",
    "            print(f\"    avg reward {avg_reward}\")\n",
    "            print(f\"    min reward {min_reward}\")\n",
    "            print(f\"    max reward {max_reward}\")\n",
    "\n",
    "        addEpisode(i,prev.unsqueeze(0),terminal.unsqueeze(0),-10,action)\n",
    "        if i%10==0:\n",
    "            target_net.load_state_dict(policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebacaf86",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fireofearth/.local/miniconda3/envs/ml7/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on episode 0\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 4.5\n",
      "    min reward 0\n",
      "    max reward 9.0\n",
      "on episode 50\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 23.34\n",
      "    min reward 11.0\n",
      "    max reward 77.0\n",
      "on episode 100\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 22.64\n",
      "    min reward 9.0\n",
      "    max reward 56.0\n",
      "on episode 150\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 22.14\n",
      "    min reward 8.0\n",
      "    max reward 52.0\n",
      "on episode 200\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 21.52\n",
      "    min reward 10.0\n",
      "    max reward 60.0\n",
      "on episode 250\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 23.32\n",
      "    min reward 9.0\n",
      "    max reward 73.0\n",
      "on episode 300\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 24.3\n",
      "    min reward 8.0\n",
      "    max reward 79.0\n",
      "on episode 350\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 24.12\n",
      "    min reward 9.0\n",
      "    max reward 80.0\n",
      "on episode 400\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 24.64\n",
      "    min reward 10.0\n",
      "    max reward 75.0\n",
      "on episode 450\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 24.3\n",
      "    min reward 10.0\n",
      "    max reward 93.0\n",
      "on episode 500\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 20.72\n",
      "    min reward 8.0\n",
      "    max reward 49.0\n",
      "on episode 550\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 23.1\n",
      "    min reward 11.0\n",
      "    max reward 45.0\n",
      "on episode 600\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 25.44\n",
      "    min reward 9.0\n",
      "    max reward 87.0\n",
      "on episode 650\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 25.76\n",
      "    min reward 9.0\n",
      "    max reward 126.0\n",
      "on episode 700\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 23.96\n",
      "    min reward 8.0\n",
      "    max reward 59.0\n",
      "on episode 750\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 23.6\n",
      "    min reward 9.0\n",
      "    max reward 57.0\n",
      "on episode 800\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 21.46\n",
      "    min reward 10.0\n",
      "    max reward 53.0\n",
      "on episode 850\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 23.3\n",
      "    min reward 10.0\n",
      "    max reward 71.0\n",
      "on episode 900\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 22.36\n",
      "    min reward 9.0\n",
      "    max reward 59.0\n",
      "on episode 950\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 21.62\n",
      "    min reward 9.0\n",
      "    max reward 59.0\n",
      "on episode 1000\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 23.26\n",
      "    min reward 10.0\n",
      "    max reward 59.0\n",
      "on episode 1050\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 22.48\n",
      "    min reward 10.0\n",
      "    max reward 64.0\n",
      "on episode 1100\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 20.64\n",
      "    min reward 9.0\n",
      "    max reward 48.0\n",
      "on episode 1150\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 21.82\n",
      "    min reward 8.0\n",
      "    max reward 47.0\n",
      "on episode 1200\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 22.6\n",
      "    min reward 10.0\n",
      "    max reward 45.0\n",
      "on episode 1250\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 25.48\n",
      "    min reward 9.0\n",
      "    max reward 55.0\n",
      "on episode 1300\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 22.1\n",
      "    min reward 9.0\n",
      "    max reward 78.0\n",
      "on episode 1350\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 23.66\n",
      "    min reward 10.0\n",
      "    max reward 59.0\n",
      "on episode 1400\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 22.78\n",
      "    min reward 9.0\n",
      "    max reward 55.0\n",
      "on episode 1450\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 23.22\n",
      "    min reward 10.0\n",
      "    max reward 64.0\n",
      "on episode 1500\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 25.34\n",
      "    min reward 10.0\n",
      "    max reward 95.0\n",
      "on episode 1550\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 22.38\n",
      "    min reward 10.0\n",
      "    max reward 59.0\n",
      "on episode 1600\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 23.4\n",
      "    min reward 9.0\n",
      "    max reward 95.0\n",
      "on episode 1650\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 25.34\n",
      "    min reward 9.0\n",
      "    max reward 58.0\n",
      "on episode 1700\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 21.76\n",
      "    min reward 9.0\n",
      "    max reward 63.0\n",
      "on episode 1750\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 21.26\n",
      "    min reward 9.0\n",
      "    max reward 54.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_26178/3730517129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainDRQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_26178/356347723.py\u001b[0m in \u001b[0;36mtrainDRQN\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessScreen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0maddEpisode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mtrainNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mprev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0msteps_done\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_26178/2098205792.py\u001b[0m in \u001b[0;36mtrainNet\u001b[0;34m(total_episodes)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/miniconda3/envs/ml7/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/miniconda3/envs/ml7/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/miniconda3/envs/ml7/lib/python3.7/site-packages/torch/optim/rmsprop.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    114\u001b[0m                       \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                       \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                       centered=group['centered'])\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/miniconda3/envs/ml7/lib/python3.7/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36mrmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0msquare_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcentered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainDRQN(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d1e59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
