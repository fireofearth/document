{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed890253",
   "metadata": {},
   "source": [
    "Pole balancing\n",
    "\n",
    "RL Based on:  \n",
    "<https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html>\n",
    "\n",
    "Recurrent DQN based on:  \n",
    "<https://arxiv.org/pdf/1507.06527.pdf>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09921700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numbers\n",
    "import random\n",
    "import collections\n",
    "from collections import namedtuple, deque\n",
    "import itertools\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchinfo\n",
    "\n",
    "import utility as util\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "056cc867",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=T.InterpolationMode.BICUBIC),\n",
    "                    T.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02fa7c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = util.AttrDict(\n",
    "        episodes=2_000,\n",
    "        minibatch_size=64,\n",
    "        multistep_size=20,\n",
    "        discount=0.999,\n",
    "        eps_start=0.9,\n",
    "        eps_end=0.05,\n",
    "        eps_decay=0.9995,\n",
    "        update_target_interval=10,\n",
    "        replay_memory_minlen=200,\n",
    "        replay_memory_maxlen=10_000,\n",
    "        aggregate_stats_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9180d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb9e1bf7110>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAglUlEQVR4nO3deXgc1Z3u8e+vu7UvlrV4kSxb8ga2wRvCNkuASwiYJXYIm8lGEiZchiHLJDO5MMnNcJmZzGS9DBcSIGRIhoR9JmCWYEKAGGKwLcfYeMG2vEreJK9arK3V5/7RhZGNF9luqdTV7+d5+lHVqaPuX6nst6tPVVeZcw4REUl+Ib8LEBGRxFCgi4gEhAJdRCQgFOgiIgGhQBcRCYiIXy9cXFzsKioq/Hp5EZGktGTJkl3OuZIjLfMt0CsqKqiurvbr5UVEkpKZbT7aMg25iIgEhAJdRCQgFOgiIgGhQBcRCQgFuohIQCjQRUQCQoEuIhIQSRfof9mylx+8/L7fZYiI9DtJF+grtu7n52+sp6a+ye9SRET6laQL9EvHDwFg3sqdPlciItK/JF2gDxmQyaTyAl5ZucPvUkRE+pWkC3SAyyYMZlndfrbvb/W7FBGRfiNJAz0+7PKKhl1ERA5KykAfVZLLqJIc5mnYRUTkoKQMdIjvpS/cuIe9LR1+lyIi0i8kdaB3xRyvvV/vdykiIv1C0gb6xGEDGDogU8MuIiKepA10M+PS8YOZv66B1o4uv8sREfFd0gY6xIdd2jpj/Gltg9+liIj4LqkD/ezKQgZkpelLRiIiJHmgp4VDXDJuMK+u3klHNOZ3OSIivkrqQAe4cuIQGtui/Llml9+liIj4KukD/fzRJeRlRnhh+Xa/SxER8VXSB3p6JMRlE4bwyqodtEd1touIpK6kD3SAKycOpaktylvrNOwiIqkrEIF+3qhiBmSl8aKGXUQkhQUi0OPDLoP5w6qdtHVq2EVEUlOPAt3MZprZGjOrMbM7jrB8uJm9bmZLzWy5mV2R+FKP7cqJpTS1R3lTwy4ikqKOG+hmFgbuBy4HxgM3mtn4w7p9F3jKOTcFmAP8LNGFHs+5o4ooyE7jxeXb+vqlRUT6hZ7soU8DapxzG5xzHcATwOzD+jgg35seAPR5qqaFQ8ycMETDLiKSsnoS6GVAbbf5Oq+tu7uAz5lZHfAS8NUjPZGZ3WJm1WZW3dCQ+OuvXDlxKC0dXbq2i4ikpEQdFL0R+JVzbhhwBfComX3kuZ1zDznnqpxzVSUlJQl66Q+dM7KIwpx0nl+mYRcRST09CfStQHm3+WFeW3c3A08BOOfeBjKB4kQUeCIi4RBXnDmEV1fvpLk92tcvLyLiq54E+mJgjJlVmlk68YOecw/rswX4OICZjSMe6L6Me1w9pYy2zhjzVugKjCKSWo4b6M65KHA7MA9YTfxslpVmdreZzfK6fQv4ipktAx4Hvuicc71V9LFMHT6Q8sIsnn338A8RIiLBFulJJ+fcS8QPdnZv+1636VXAeYkt7eSYGZ+aXMb9r9dQ39jGoPxMv0sSEekTgfim6OFmTy4j5mCuDo6KSAoJZKCPHpTLmWUDeO5dBbqIpI5ABjrA7MmlvLd1PzX1zX6XIiLSJwIb6LMmlRIyeE4HR0UkRQQ20AflZ3Le6GKefXcrPp1wIyLSpwIb6ACfmlxG7Z5W/rJlr9+liIj0ukAH+mVnDCEzLcQzSzTsIiLBF+hAz82IcMUZQ3lh2TZaO3QFRhEJtkAHOsB1VeU0tUeZt1KXAhCRYAt8oE+vLKS8MIunqmuP31lEJIkFPtBDIeO6s8pZsH43tXsO+F2OiEivCXygA1xz1jDM4JkldX6XIiLSa1Ii0MsKsjh/dDHPLKkjFtM56SISTCkR6ADXnjWMrftaeXvDbr9LERHpFSkT6JdNGEJ+ZoSndXBURAIqZQI9My3MrMml/H7FDva3dvpdjohIwqVMoANcX1VOezTGXF2wS0QCKKUC/cyyAUwozee3C7fogl0iEjgpFehmxmenj+D9HU38Zcs+v8sREUmolAp0gFmTS8nNiPDbhZv9LkVEJKFSLtBzMyJ8akopLyzfzr4DHX6XIyKSMCkX6ACfmTaCjmhM3xwVkUBJyUAfX5rPlOEFPLZIB0dFJDhSMtABPjt9BBsaWnhnwx6/SxERSYiUDfSrJg4lP1MHR0UkOFI20DPTwlx7VjnzVu6goand73JERE5ZygY6wGdnDKezy/HYwi1+lyIicspSOtBHleRy4dgSfrNwMx3RmN/liIickpQOdIAvnVdBQ1M7L7233e9SREROScoH+gVjShhZnMMjf96oUxhFJKmlfKCHQsYXz6tgWd1+Xd9FRJJaygc6wDVTh5GXEeFXCzb5XYqIyElToAM5GRGuP7uc37+3nR372/wuR0TkpCjQPTedU0GXczz6zia/SxEROSkKdM/womwuGTeYxxZuoa2zy+9yREROWI8C3cxmmtkaM6sxszuO0ud6M1tlZivN7LHEltk3bj6/kr0HOnUVRhFJSscNdDMLA/cDlwPjgRvNbPxhfcYAdwLnOecmAN9IfKm9b3plIZPKC/jFmxvoiukURhFJLj3ZQ58G1DjnNjjnOoAngNmH9fkKcL9zbi+Ac64+sWX2DTPj1gtGsnn3Aeat3OF3OSIiJ6QngV4G1Habr/PauhsLjDWzP5vZO2Y2M1EF9rVLJwyhoiibB/+0Xl80EpGkkqiDohFgDHARcCPwCzMrOLyTmd1iZtVmVt3Q0JCgl06scMj4ygUjWVa3X9dKF5Gk0pNA3wqUd5sf5rV1VwfMdc51Ouc2AmuJB/whnHMPOeeqnHNVJSUlJ1tzr7tm6jCKc9N5cP56v0sREemxngT6YmCMmVWaWTowB5h7WJ9nie+dY2bFxIdgNiSuzL6VmRbmi+dW8MaaBlZvb/S7HBGRHjluoDvnosDtwDxgNfCUc26lmd1tZrO8bvOA3Wa2Cngd+Hvn3O7eKrovfG7GCLLTw/xiftK+L4lIijG/DvxVVVW56upqX167p+5+fhW/fnsTb/zdRZQXZvtdjogIZrbEOVd1pGX6pugx3HLBSMJm/OwNjaWLSP+nQD+GIQMyueHscp5ZUsvWfa1+lyMickwK9OO49aJRADygvXQR6ecU6MdRVpDFtWeV8+TiWl1aV0T6NQV6D9x20Si6nNN56SLSrynQe6C8MJtPTynjsYVbqG/SXrqI9E8K9B76m/8xms6umM5LF5F+S4HeQxXFOXxqchmPvrOZ+kbtpYtI/6NAPwFfv2QM0S7Hfa/X+F2KiMhHKNBPwIiiHG44u5zHF22hds8Bv8sRETmEAv0EffXiMYTMuOfVdX6XIiJyCAX6CRoyIJObzq3gd0vrWLezye9yREQOUqCfhFsvHEV2eoSfvLLW71JERA5SoJ+Ewpx0/upjlby8cgfLavf5XY6ICKBAP2k3n1/JwOw0fjRvje49KiL9ggL9JOVlpnH7xWN4q2YXb6ztn/dHFZHUokA/BZ+fMYKKomy+/+Jqol0xv8sRkRSnQD8F6ZEQd1x+Ouvqm3myutbvckQkxSnQT9FlE4YwraKQ//uHtTS1dfpdjoikMAX6KTIzvnPlOHY1d/DAn3R5XRHxjwI9ASaVF/CpyaU8/OZGtulWdSLiEwV6gvz9zNMB+MHL7/tciYikKgV6gpQVZHHLBSN57t1tLNq4x+9yRCQFKdAT6LaLRlNWkMX3nluh0xhFpM8p0BMoKz3M/75qHO/vaOLRdzb7XY6IpBgFeoJdNmEIHxtTzE9fWUtDU7vf5YhIClGgJ5iZcdesCbRFu3SAVET6lAK9F4wqyeXm80fyzJI6lmze63c5IpIiFOi95KsXj2ZIfibffXYFnTpAKiJ9QIHeS3IyItw1awKrtzfy8Jsb/S5HRFKAAr0XzTxjCJdNGMw9r65l064Wv8sRkYBToPeyu2efQXo4xD/87j3dCENEepUCvZcNzs/kf11+OgvW7+bpJXV+lyMiAaZA7wOfmTacsysG8i8vrta56SLSaxTofSAUMv7102fS2tHFXc+v9LscEQkoBXofGT0oj699fDQvLt/OC8u3+V2OiARQjwLdzGaa2RozqzGzO47R7xozc2ZWlbgSg+PWC0cxadgAvvvsCuqb2vwuR0QC5riBbmZh4H7gcmA8cKOZjT9Cvzzg68DCRBcZFJFwiJ9cP5nWji7u/C+d9SIiidWTPfRpQI1zboNzrgN4Aph9hH7/BPwA0K7nMYwelMu3Z57OH9+v11kvIpJQPQn0MqD7Le3rvLaDzGwqUO6ce/FYT2Rmt5hZtZlVNzQ0nHCxQfGlcyuYXlnI3c+vom7vAb/LEZGAOOWDomYWAn4KfOt4fZ1zDznnqpxzVSUlJaf60kkrFDJ+fN0knHP8/dPL6Ypp6EVETl1PAn0rUN5tfpjX9oE84AzgDTPbBMwA5urA6LGVF2bzj5+cwNsbdvPAn9b7XY6IBEBPAn0xMMbMKs0sHZgDzP1goXNuv3Ou2DlX4ZyrAN4BZjnnqnul4gC5rmoYV00cyk//sFaX2RWRU3bcQHfORYHbgXnAauAp59xKM7vbzGb1doFBZmZ8/9NnMnRAJl97fCn7Wzv9LklEkpj5depcVVWVq67WTjzAX7bs5boH3mbmGUO478YpmJnfJYlIP2VmS5xzRxzS1jdF+4GpwwfyzU+M5cXl23mquvb4vyAicgQK9H7i1gtHce6oIv5x7kpWb2/0uxwRSUIK9H4iHDLumTOZ/Mw0bv3NEo2ni8gJU6D3I4PyMvnZZ6eydW8r33rqXWI6P11EToACvZ+pqijku1eO49XV9fzsjRq/yxGRJKJA74duOreC2ZNL+ckf1jJ/bepeIkFETowCvR8yi98QY+ygPL7+xFJq9+h6LyJyfAr0fio7PcIDnz+LmIObf72YpjYdJBWRY1Og92OVxTn8/LNT2dDQwtceX6qLeInIMSnQ+7lzRxdz16wJvL6mgX99abXf5YhIPxbxuwA5vs/NGEFNfTMPv7WR0YNymTNtuN8liUg/pD30JPHdK8fxsTHFfPfZFSyo2eV3OSLSDynQk0QkHOK+z0xlZEkOtzy6hFXbdHkAETmUAj2JDMhK41dfmkZuRoQvPrJIpzOKyCEU6EmmtCCL/7x5Gm2dXdz0yCL2tnT4XZKI9BMK9CQ0dnAeD990NnV7W/nyrxfT2tHld0ki0g8o0JPUtMpC7p0zmXdr93Hbb5fQEY35XZKI+EyBnsRmnjGUf/nUmby+poGvPb6UaJdCXSSVKdCT3GemD+d7V43n5ZU7+NbTy/RtUpEUpi8WBcCXz6+kLdrFD19eQ1ZamO9ffSahkO5LKpJqFOgBcdtFo2nr6OLe12rIiIS4a9YE3WxaJMUo0APkbz8xlrZojIfmbyAac/zT7DO0py6SQhToAWJm3Hn56YRDxs/fWE9HNMa/XTORsEJdJCUo0APGzPj2ZaeREQlxz6vr6OiK8ZPrJhEJ6/i3SNAp0APIzPjGJWPJiIT5wcvv094Z494bp5AeUaiLBJn+hwfYX1806uApjTf/ejHN7VG/SxKRXqRAD7gvn1/JD6+dyIL1u5nz0Ns0NLX7XZKI9BIFegq4vqqcX3zhLGrqm7nm5wvYtKvF75JEpBco0FPExacP5vGvzKCprZNrfr6A5XX7/C5JRBJMgZ5CpgwfyDN/fS5Z6WFuePAdXl6xw++SRCSBFOgpZlRJLv9927mcNiSPW3+zhPtfr8E5Xf9FJAgU6CloUF4mT9wyg1mTSvnRvDX87ZPv0tapa6qLJDudh56iMtPC/PucyYwdnMuPX1nL5j0HePDzZzEoL9Pv0kTkJGkPPYWZGbdfPIYHPjeV97c3cdW9b7F40x6/yxKRk6RAF2aeMZTf/c255GREmPPQOzz85gaNq4skoR4FupnNNLM1ZlZjZnccYfk3zWyVmS03sz+a2YjElyq96fQh+Tx3+3lcMm4Q//ziav7msb/Q1Nbpd1kicgKOG+hmFgbuBy4HxgM3mtn4w7otBaqccxOBZ4AfJrpQ6X35mWk88Lmz+IcrTmfeyp3Mvu/PrNy23++yRKSHerKHPg2occ5tcM51AE8As7t3cM697pw74M2+AwxLbJnSV8yMWy4YxWN/NZ3m9ihX37+Ah9/cQEy3thPp93oS6GVAbbf5Oq/taG4Gfn+kBWZ2i5lVm1l1Q0NDz6uUPjd9ZBEvf+MCLjythH9+cTU3PbKInY1tfpclIseQ0IOiZvY5oAr40ZGWO+cecs5VOeeqSkpKEvnS0gsKc9J56PNn8f2rz2Txpj3MvGc+r6zUt0tF+queBPpWoLzb/DCv7RBmdgnwHWCWc06X9AsIM+Mz04fzwlc/RmlBFrc8uoRvPvku+w50+F2aiBymJ4G+GBhjZpVmlg7MAeZ272BmU4AHiYd5feLLFL+NHpTL7247j69dPJq5y7ZxyU/n61owIv3McQPdORcFbgfmAauBp5xzK83sbjOb5XX7EZALPG1m75rZ3KM8nSSx9EiIb156Gs/dfh6D8jK49TdLuP2xv7C7WR/IRPoD8+sLJFVVVa66utqX15ZT19kV44E31nPva+vIy0zjjstP59qpwwjphtQivcrMljjnqo60TN8UlZOSFg7x1Y+P4YWvfozK4hy+/cxyrnvwbVZta/S7NJGUpUCXU3LakDye/p/n8MNrJ7JxVwufvO8t7n5+lb5lKuIDBbqcslDIuL6qnNe+dSE3nF3OIws28vGf/ImnFtfSpS8kifQZBbokTEF2Ot+/+kyeve08Sguy+PZ/LefKe9/kzXX6EplIX1CgS8JNKi/gd7edy/+7cQrN7VE+/8tFfPGRRazd2eR3aSKBpkCXXmFmfHJSKX/81oV854pxLNm8l5n3zOfvnl5G7Z4Dx38CETlhOm1R+sTelg7ue72GR9/ZTCzmuK6qnNsvHk1ZQZbfpYkklWOdtqhAlz61s7GN+1+v4YlF8eu9zZlWzm0XjWbIAN36TqQnFOjS72zd18p9r9XwdHUtITM+PbWMWy4YyciSXL9LE+nXFOjSb9XuOcCD89fzdHUdHV0xLhs/hFsvGsXk8gK/SxPplxTo0u81NLXzqwUbefTtzTS2RZkxspCvfGwkF502iLAuJyBykAJdkkZze5THF27hl29tZEdjG8MLs/n8jBFcX1XOgOw0v8sT8Z0CXZJOZ1eMeSt38J8LNrNo0x4y00JcPaWML5xTwbih+X6XJ+IbBboktZXb9vPo25t59t2ttHXGmDK8gBuqyrly4lDyMrXXLqlFgS6BsO9AB09X1/FkdS019c1kpYW54syhXF81jGmVhZhprF2CT4EugeKc493afTxVXcvzy7bT3B6loiibq6cMY9bkUiqLc/wuUaTXKNAlsA50RPn9ezt4qrqWRZv24BycUZbPrEmlXDWxlFJ9E1UCRoEuKWH7/lZeXL6ducu2sbxuPwBnVwzkqomlXDphMEMHKNwl+SnQJeVs2tXC88u2MXfZNtbVNwMwcdgAPjFuMJdOGMLYwbkac5ekpECXlFZT38wfVu3klVU7WLplHwAjirL5xLjBXDxuEFUjCkmP6MKjkhwU6CKe+sY2Xl1dzyurdrCgZjcdXTGy08OcM7KIC8aWcOHYEip0UFX6MQW6yBE0t0d5e/1u5q9tYP66Bjbvjl+nfXhhNheMLea8UcVMqyykKDfD50pFPqRAF+mBTbtamL+ugflrG1iwfjcHOroAGDs4l+mVRUwfWcj0yiJK8hTw4h8FusgJ6ojGeG/rft7ZsJuFG/ewZNMeWryAH1mSw/TKQqYMH8jU4QWMLM4lpAuISR9RoIucomhXjBXbGlnoBfziTXtoaosCkJcZYXJ5AVPKC5gyfCCTywsYmJPuc8USVAp0kQSLxRwbdrWwdMteltbuY+mWfazZ0UjM++80oiibCaX5TCgdwPih+UwozWdQvu7KJKfuWIEe6etiRIIgFDJGD8pl9KBcrqsqB6ClPcp7W/ezdMs+ltftY+W2Rl56b8fB3ynOzWB8aTzcxw/N5/QheYwoytEpk5IwCnSRBMnJiDBjZBEzRhYdbGts62T1tkZWbW9k5bb44xfzNxD1duUjIaOiOIexg3MZPSiPMYNyGTs4j4ribDIiYb9WRZKUAl2kF+VnpjF9ZBHTu4V8e7SLdTubqalvZl19E2t3NrN6exMvr9hxcMgmHDJGFGUzsjiHEUU5VBRlM6Ioh8riHIYOyCQS1l69fJQCXaSPZUTCnFE2gDPKBhzS3tbZxcZdLayrb2bdzibW7Wxm0+4W3qrZRVtn7GC/tLBRPjCbEV7IjyjKZtjAbEoLMhlWkE1+VkSXNUhRCnSRfiIzLcy4ofkfuSOTc476pnY27Wph8+4DbNzdwubdLWzadYBFGz88nfIDuRkRSgsyKSvIomxgFmUFXtgPzGLIgCxKcjM0bh9QCnSRfs7MGJyfyeD8zEOGbiAe9rtbOti6t5Wt+1rZtq+Vum7TS2v3se9A50eeszAnnUF5GQzKz2RwXgaD8jMYnJ95sG1QXgYleRkax08yCnSRJGZmFOdmUJybwaTygiP2aWmPxoN+Xys797exs7Gd+qb4z4amNtbuaKKhuZ2u2EdPYc7LjFCUk05hTjqFORnx6dz0bm3pFOVkHGzLTNMbgJ8U6CIBl5MRYczgPMYMzjtqn66YY09LBzsb26hvaqO+sZ36pnb2tHSwu6WDPS3t1O09wPK6fexp6Th4ls7hstLC5GdFGJCVdvCR/8HPzLRD2gdkf9ienxUhKy2ssf9TpEAXEcIho8QbZoEBx+zrnKOxLcoeL+h3N3ccDP69LR00tnWyvzX+2LqvjdXbm2hs7aSpPXrM5w1Z/M0nNyNCjvfIzQiTkx4hN/PD9tyMCDnpYXIyIuRlxtuy0+NvCFnp4fjPtDCZ6SHSw6GUepPoUaCb2Uzg34Ew8LBz7t8OW54B/CdwFrAbuME5tymxpYpIf2BmB/eyT+T+rdGuGE1tUfa3dh4S+vtbO2lsjdLSHqW5Pf6zpSNKc3sXzW2d7GrqiLd3RGluix7108GRhIyDQZ/pBf0H0/H50EfaMiIh0iPxN4OMSIiMSDg+77WlR0If9okc1qfbcj9OLT1uoJtZGLgf+ARQByw2s7nOuVXdut0M7HXOjTazOcAPgBt6o2ARSU6RcIiBOemndJ0b5xzt0Vg89Nu7Pgz69ijtnV20dnbR2hGjtbOLts4uWju8n97jg7bWzi72t3ayc3+3ZV77ibxhHEvI6BbyYdLCRlo4RFrY+MYlY/nkpNKEvE53PdlDnwbUOOc2AJjZE8BsoHugzwbu8qafAe4zM3N+XShGRALJzA7uSRfl9s5rxGKOjq4Y7dEYHdEY7dEuOqIxOro+mI//PDjdFaO9s+vg8kP6dH34szMao7MrRmfMUZCd1iu19yTQy4DabvN1wPSj9XHORc1sP1AE7OreycxuAW4BGD58+EmWLCLSe0IhIzMUTsozdvp0kMc595Bzrso5V1VSUtKXLy0iEng9CfStQHm3+WFe2xH7mFmE+GHy3YkoUEREeqYngb4YGGNmlWaWDswB5h7WZy5wkzd9LfCaxs9FRPrWccfQvTHx24F5xE9b/A/n3Eozuxuods7NBX4JPGpmNcAe4qEvIiJ9qEfnoTvnXgJeOqzte92m24DrEluaiIicCF1yTUQkIBToIiIBoUAXEQkI8+tkFDNrADaf5K8Xc9iXllKA1jk1aJ1Tw6ms8wjn3BG/yONboJ8KM6t2zlX5XUdf0jqnBq1zauitddaQi4hIQCjQRUQCIlkD/SG/C/CB1jk1aJ1TQ6+sc1KOoYuIyEcl6x66iIgcRoEuIhIQSRfoZjbTzNaYWY2Z3eF3PSfLzMrN7HUzW2VmK83s6157oZn9wczWeT8Heu1mZvd6673czKZ2e66bvP7rzOymo71mf2FmYTNbamYvePOVZrbQW7cnvat6YmYZ3nyNt7yi23Pc6bWvMbPLfFqVHjGzAjN7xszeN7PVZnZO0Lezmf2t9+96hZk9bmaZQdvOZvYfZlZvZiu6tSVsu5rZWWb2nvc795r14G7XzrmkeRC/2uN6YCSQDiwDxvtd10muy1BgqjedB6wFxgM/BO7w2u8AfuBNXwH8HjBgBrDQay8ENng/B3rTA/1ev+Os+zeBx4AXvPmngDne9APAX3vTtwEPeNNzgCe96fHets8AKr1/E2G/1+sY6/tr4K+86XSgIMjbmfgdzDYCWd227xeDtp2BC4CpwIpubQnbrsAir695v3v5cWvy+49ygn/Ac4B53ebvBO70u64ErdtzxG/EvQYY6rUNBdZ40w8CN3brv8ZbfiPwYLf2Q/r1twfxG6T8EbgYeMH7x7oLiBy+jYlfsvkcbzri9bPDt3v3fv3tQfxmLxvxTkA4fPsFcTvz4S0pC73t9gJwWRC3M1BxWKAnZLt6y97v1n5Iv6M9km3I5Uj3Ny3zqZaE8T5iTgEWAoOdc9u9RTuAwd700dY92f4m9wDfBmLefBGwzzkX9ea713/IvWqBD+5Vm0zrXAk0AI94w0wPm1kOAd7OzrmtwI+BLcB24tttCcHezh9I1HYt86YPbz+mZAv0wDGzXOC/gG845xq7L3Pxt+bAnFdqZlcB9c65JX7X0ocixD+W/9w5NwVoIf5R/KAAbueBwGzib2alQA4w09eifODHdk22QO/J/U2ThpmlEQ/z3zrn/ttr3mlmQ73lQ4F6r/1o655Mf5PzgFlmtgl4gviwy78DBRa/Fy0cWv/R7lWbTOtcB9Q55xZ6888QD/ggb+dLgI3OuQbnXCfw38S3fZC38wcStV23etOHtx9TsgV6T+5vmhS8I9a/BFY7537abVH3+7PeRHxs/YP2L3hHy2cA+72PdvOAS81soLdndKnX1u845+50zg1zzlUQ33avOec+C7xO/F608NF1PtK9aucCc7yzIyqBMcQPIPU7zrkdQK2ZneY1fRxYRYC3M/Ghlhlmlu39O/9gnQO7nbtJyHb1ljWa2Qzvb/iFbs91dH4fVDiJgxBXED8jZD3wHb/rOYX1OJ/4x7HlwLve4wriY4d/BNYBrwKFXn8D7vfW+z2gqttzfRmo8R5f8nvderj+F/HhWS4jif9HrQGeBjK89kxvvsZbPrLb73/H+1usoQdH/31e18lAtbetnyV+NkOgtzPwf4D3gRXAo8TPVAnUdgYeJ36MoJP4J7GbE7ldgSrv77ceuI/DDqwf6aGv/ouIBESyDbmIiMhRKNBFRAJCgS4iEhAKdBGRgFCgi4gEhAJdRCQgFOgiIgHx/wElJECPuj9ZVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10_000\n",
    "l = [None]*n\n",
    "l[0] = config.eps_start\n",
    "for i in range(1, n):\n",
    "    l[i] = l[i - 1]*config.eps_decay\n",
    "plt.plot(np.arange(10_000), l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e9ad01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transitions and Replay Memory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('curr_state', 'action', 'next_state', 'reward', 'is_done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity, transition=Transition):\n",
    "        self.__episode_to_frames = collections.OrderedDict()\n",
    "        self.__n_frames = 0\n",
    "        self.__capacity = capacity\n",
    "        self.__transition = transition\n",
    "\n",
    "    def add(self, episode, tuple_args):\n",
    "        \"\"\"Save a transition.\n",
    "        Assumes that the number of frames << capacity.\n",
    "        \"\"\"\n",
    "        if episode in self.__episode_to_frames:\n",
    "            self.__episode_to_frames[episode].append(self.__transition(*tuple_args))\n",
    "        else:\n",
    "            self.__episode_to_frames[episode] = [self.__transition(*tuple_args)]\n",
    "        self.__n_frames += 1\n",
    "        if self.__n_frames > self.__capacity:\n",
    "            _episode, _frames = self.__episode_to_frames.popitem(last=False)\n",
    "            self.__n_frames -= len(_frames)\n",
    "\n",
    "    def __f(self, frames):\n",
    "        return self.__transition(*[ torch.stack(x) for x in zip(*frames) ])\n",
    "            \n",
    "    def sample(self, batch_size, steps):\n",
    "        # choice_episodes_frames is a batch_size sized list of (list of Transition)\n",
    "        choice_episodes_frames = random.choices(\n",
    "                list(self.__episode_to_frames.values()), k=batch_size)\n",
    "        # start_indices is a batch_size sized list of int \n",
    "        start_indices = [\n",
    "                random.randrange(0, max(len(val) - steps + 1, 1)) for val in choice_episodes_frames]\n",
    "        choice_episodes_frames = [\n",
    "                choice_episodes_frames[idx][start_idx:start_idx+steps] \\\n",
    "                    for idx, start_idx in enumerate(start_indices)]\n",
    "        return [self.__f(_frames) for _frames in choice_episodes_frames]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.__n_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56ac767a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A(a=tensor([0, 2, 4]), b=tensor([1, 3, 5])),\n",
      " A(a=tensor([10, 12, 14]), b=tensor([11, 13, 15])),\n",
      " A(a=tensor([14, 16, 18]), b=tensor([15, 17, 19]))]\n",
      "OrderedDict([(1,\n",
      "              [A(a=tensor(0), b=tensor(1)),\n",
      "               A(a=tensor(2), b=tensor(3)),\n",
      "               A(a=tensor(4), b=tensor(5)),\n",
      "               A(a=tensor(6), b=tensor(7)),\n",
      "               A(a=tensor(8), b=tensor(9))]),\n",
      "             (2,\n",
      "              [A(a=tensor(10), b=tensor(11)),\n",
      "               A(a=tensor(12), b=tensor(13)),\n",
      "               A(a=tensor(14), b=tensor(15)),\n",
      "               A(a=tensor(16), b=tensor(17)),\n",
      "               A(a=tensor(18), b=tensor(19))])])\n",
      "OrderedDict([(2,\n",
      "              [A(a=tensor(10), b=tensor(11)),\n",
      "               A(a=tensor(12), b=tensor(13)),\n",
      "               A(a=tensor(14), b=tensor(15)),\n",
      "               A(a=tensor(16), b=tensor(17)),\n",
      "               A(a=tensor(18), b=tensor(19))]),\n",
      "             (3,\n",
      "              [A(a=tensor(20), b=tensor(21)),\n",
      "               A(a=tensor(22), b=tensor(23)),\n",
      "               A(a=tensor(24), b=tensor(25))])])\n"
     ]
    }
   ],
   "source": [
    "# Test ReplayMemory\n",
    "_Transition = namedtuple('A',\n",
    "                        ('a', 'b'))\n",
    "memory = ReplayMemory(10, transition=_Transition)\n",
    "inputs = torch.arange(10).reshape(-1,2)\n",
    "for input in inputs:\n",
    "    memory.add(1, input)\n",
    "inputs = torch.arange(10, 20).reshape(-1,2)\n",
    "for input in inputs:\n",
    "    memory.add(2, input)\n",
    "pp.pprint(memory.sample(3, 3))\n",
    "pp.pprint(memory._ReplayMemory__episode_to_frames)\n",
    "inputs = torch.arange(20, 26).reshape(-1,2)\n",
    "for input in inputs:\n",
    "    memory.add(3, input)\n",
    "pp.pprint(memory._ReplayMemory__episode_to_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "163a9b5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment attributes: gravity masscart masspole total_mass length polemass_length force_mag tau kinematics_integrator theta_threshold_radians x_threshold action_space observation_space np_random viewer state steps_beyond_done spec\n",
      "Gravitational constant: 9.8\n",
      "Number of possible actions an agent can make: 2\n",
      "Sampling of discrete action space gives: 0\n",
      "Current state is: None\n",
      "States are bounded below by: [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n",
      "States are bounded above by: [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "# Some attributes of Gym environment\n",
    "# Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "print(\"Environment attributes:\", *vars(env).keys())\n",
    "print(\"Gravitational constant:\", env.gravity)\n",
    "print(\"Number of possible actions an agent can make:\", env.action_space.n)\n",
    "print(\"Sampling of discrete action space gives:\", env.action_space.sample())\n",
    "# The (observation) state is an array\n",
    "# [Cart Position, Cart Velocity, Pole Angle, Pole Angular Velocity]\n",
    "print(\"Current state is:\", env.state)\n",
    "print(\"States are bounded below by:\", env.observation_space.low)\n",
    "print(\"States are bounded above by:\",env.observation_space.high)\n",
    "# help(env.observation_space)\n",
    "# help(env) # for more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8638abe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faaa0849f90>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Getting the first snapshot of the environment after resetting the Gym \n",
    "env.reset()\n",
    "screen = env.render(mode='rgb_array')\n",
    "plt.imshow(screen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7ec41fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_cart_location(env, screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def preprocess_screen(env, screen):\n",
    "    # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "    # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "    screen = screen.transpose((2, 0, 1))\n",
    "    # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(env, screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescale, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "def render(env):\n",
    "    return env.render(mode='rgb_array')\n",
    "\n",
    "env.reset()\n",
    "screen = render(env)\n",
    "screen = preprocess_screen(env, screen)\n",
    "plt.imshow(screen.cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97caa60a",
   "metadata": {},
   "source": [
    "## Using CartPole from PyTorch Tutorial\n",
    "\n",
    "Based on:\n",
    "<https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bc0d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentDQNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, channels, height, width, outputs):\n",
    "        super().__init__()\n",
    "        # nn.BatchNorm2d produces nans if the input is a nan since it keeps a moving average\n",
    "        self.convs = nn.Sequential(*[\n",
    "                nn.Conv2d(3, 16, kernel_size=5, stride=2),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(16, 32, kernel_size=5, stride=2),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(32, 32, kernel_size=5, stride=2),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Flatten()])\n",
    "        self.lstm = nn.LSTM(512, 128, 1, batch_first=True)\n",
    "        self.linear = nn.Linear(128, outputs)\n",
    "        self.__memory = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.__memory = None\n",
    "\n",
    "    def forward(self, x, is_optimizing=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ==========\n",
    "        x : tensor\n",
    "            Has shape (batch size, history, channels, height, width)\n",
    "        is_training : bool\n",
    "            Is training\n",
    "        \n",
    "        Returns\n",
    "        =======\n",
    "        tensor\n",
    "            (batch size, history, actions)\n",
    "        \"\"\"\n",
    "        # for processed CartPole scene\n",
    "        # x has shape (n_batch, n_history, 3, 40, 90)\n",
    "        n_batch = x.shape[0]\n",
    "        n_history = x.shape[1]\n",
    "        # after passing through convolutions\n",
    "        # x has shape (n_batch*n_history, 32, 2, 8)\n",
    "        x = x.reshape(n_batch*n_history, *x.shape[2:])\n",
    "        x = self.convs(x)\n",
    "        # x has shape (n_batch, n_history, 512)\n",
    "        x = x.reshape(n_batch, n_history, -1)\n",
    "        if is_optimizing:\n",
    "            x, _ = self.lstm(x)\n",
    "            # if is_optimizing and self.training: print(h)\n",
    "        else:\n",
    "            x, self.__memory = self.lstm(x, self.__memory)\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a32c597e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape        Output Shape       Param #            Kernel Shape\n",
       "================================================================================================================\n",
       "RecurrentDQNet                           --                 --                 --                 --\n",
       "├─Sequential: 1-1                        [1, 3, 40, 90]     [1, 512]           --                 --\n",
       "│    └─Conv2d: 2-1                       [1, 3, 40, 90]     [1, 16, 18, 43]    1,216              [3, 16, 5, 5]\n",
       "│    └─BatchNorm2d: 2-2                  [1, 16, 18, 43]    [1, 16, 18, 43]    32                 [16]\n",
       "│    └─ReLU: 2-3                         [1, 16, 18, 43]    [1, 16, 18, 43]    --                 --\n",
       "│    └─Conv2d: 2-4                       [1, 16, 18, 43]    [1, 32, 7, 20]     12,832             [16, 32, 5, 5]\n",
       "│    └─BatchNorm2d: 2-5                  [1, 32, 7, 20]     [1, 32, 7, 20]     64                 [32]\n",
       "│    └─ReLU: 2-6                         [1, 32, 7, 20]     [1, 32, 7, 20]     --                 --\n",
       "│    └─Conv2d: 2-7                       [1, 32, 7, 20]     [1, 32, 2, 8]      25,632             [32, 32, 5, 5]\n",
       "│    └─BatchNorm2d: 2-8                  [1, 32, 2, 8]      [1, 32, 2, 8]      64                 [32]\n",
       "│    └─ReLU: 2-9                         [1, 32, 2, 8]      [1, 32, 2, 8]      --                 --\n",
       "│    └─Flatten: 2-10                     [1, 32, 2, 8]      [1, 512]           --                 --\n",
       "├─LSTM: 1-2                              [1, 1, 512]        [1, 1, 128]        328,704            --\n",
       "├─Linear: 1-3                            [1, 1, 128]        [1, 1, 2]          258                [128, 2]\n",
       "================================================================================================================\n",
       "Total params: 368,802\n",
       "Trainable params: 368,802\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 3.48\n",
       "================================================================================================================\n",
       "Input size (MB): 0.04\n",
       "Forward/backward pass size (MB): 0.28\n",
       "Params size (MB): 1.48\n",
       "Estimated Total Size (MB): 1.80\n",
       "================================================================================================================"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = RecurrentDQNet(3, 40, 90, 2)\n",
    "col_names = (\"input_size\", \"output_size\", \"num_params\", \"kernel_size\")#, \"mult_adds\",)\n",
    "torchinfo.summary(net, input_size=(1, 1, 3, 40, 90), col_names=col_names, col_width=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e1830e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentStateDQNAgent(object):\n",
    "    \"\"\"\n",
    "    Trains a 1-step DQN agent with LSTM using bootstrapped random updates. \n",
    "    \n",
    "    > Episodes are selected randomly from the replay memory and updates \n",
    "    > begin at random points in the episode and proceed for only unroll\n",
    "    > iterations timesteps (e.g. one backward call).\n",
    "    \n",
    "    Based on:\n",
    "    https://arxiv.org/pdf/1507.06527.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __update_target_model(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.terminal_state_counter = 0\n",
    "    \n",
    "    def __init__(self, env, config, device):\n",
    "        self.env      = env\n",
    "        self.config   = config\n",
    "        self.device   = device\n",
    "        self.discount = self.config.discount\n",
    "        self.eps_end  = self.config.eps_end\n",
    "        self.eps_start = self.config.eps_start\n",
    "        self.eps_decay = self.config.eps_decay\n",
    "        self.epsilon   = self.eps_start\n",
    "        self.replay_memory_minlen = self.config.replay_memory_minlen\n",
    "        self.replay_memory_maxlen = self.config.replay_memory_maxlen\n",
    "        self.update_target_interval = self.config.update_target_interval\n",
    "        self.multistep_size = self.config.multistep_size\n",
    "        self.minibatch_size = self.config.minibatch_size\n",
    "        self.env.reset()\n",
    "        init_screen = self.render(is_input=True)\n",
    "        self.n_channels, self.screen_height, self.screen_width = init_screen.shape\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.policy_net = RecurrentDQNet(\n",
    "                self.n_channels, self.screen_height, self.screen_width, self.n_actions).to(self.device)\n",
    "        self.target_net = RecurrentDQNet(\n",
    "                self.n_channels, self.screen_height, self.screen_width, self.n_actions).to(self.device)\n",
    "        self.target_net.eval()\n",
    "        self.__update_target_model()\n",
    "        self.optimizer = optim.RMSprop(self.policy_net.parameters())\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "        self.replay_memory = ReplayMemory(self.replay_memory_maxlen)\n",
    "        self.steps = 0\n",
    "\n",
    "    def update_replay_memory(self, episode, tuple_args):\n",
    "        \"\"\"\n",
    "        Adds step's data to a memory replay array\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        transition : tuple\n",
    "            Contains (curr_state, action, next_state, reward, is_done) where\n",
    "            - curr_state : or s_t, an RGB image of dimensions (channels, height, width)\n",
    "            - action     : or a_t, an integer 0-9.\n",
    "            - next_state : or s_{t+1}, an RGB image of dimensions (channels, height, width)\n",
    "            - reward     : or r_t, a number\n",
    "            - is_done    : a boolean specifying whether s_t is a terminal state\n",
    "                           where the simulation ended.\n",
    "        \"\"\"\n",
    "        self.replay_memory.add(episode, tuple_args)\n",
    "\n",
    "    def select_action(self, state, is_training=True):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.epsilon\n",
    "        if is_training:\n",
    "            self.epsilon = max(self.epsilon*self.eps_decay, self.eps_end)\n",
    "        if is_training and sample <= eps_threshold:\n",
    "            return torch.tensor(random.randrange(self.n_actions), device=self.device, dtype=torch.long)\n",
    "        else:\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad():\n",
    "                values, indices = self.policy_net(state.unsqueeze(0).unsqueeze(0)).max(2)\n",
    "            self.policy_net.train()\n",
    "            return indices.squeeze()\n",
    "\n",
    "    def __get_cart_location(self, screen_width):\n",
    "        world_width = self.env.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        return int(self.env.state[0] * scale + screen_width / 2.0)\n",
    "\n",
    "    def __preprocess_screen(self, screen):\n",
    "        # Returned screen requested by gym is 400x600x3, but is sometimes larger\n",
    "        # such as 800x1200x3. Transpose it into torch order (CHW).\n",
    "        screen = screen.transpose((2, 0, 1))\n",
    "        # Cart is in the lower half, so strip off the top and bottom of the screen\n",
    "        _, screen_height, screen_width = screen.shape\n",
    "        screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "        view_width = int(screen_width * 0.6)\n",
    "        cart_location = self.__get_cart_location(screen_width)\n",
    "        if cart_location < view_width // 2:\n",
    "            slice_range = slice(view_width)\n",
    "        elif cart_location > (screen_width - view_width // 2):\n",
    "            slice_range = slice(-view_width, None)\n",
    "        else:\n",
    "            slice_range = slice(cart_location - view_width // 2,\n",
    "                                cart_location + view_width // 2)\n",
    "        # Strip off the edges, so that we have a square image centered on a cart\n",
    "        screen = screen[:, :, slice_range]\n",
    "        # Convert to float, rescale, convert to torch tensor\n",
    "        # (this doesn't require a copy)\n",
    "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "        screen = torch.from_numpy(screen)\n",
    "        # Resize, and add a batch dimension (BCHW)\n",
    "        return resize(screen).to(self.device)\n",
    "\n",
    "    def render(self, is_input=False):\n",
    "        screen = self.env.render(mode='rgb_array')\n",
    "        if is_input:\n",
    "            return self.__preprocess_screen(screen)\n",
    "        else:\n",
    "            return screen\n",
    "\n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        self.policy_net.reset()\n",
    "    \n",
    "    def __pad_transitions(self, transitions):\n",
    "        \"\"\"Pad transitions that are not length multistep_size\n",
    "        \"\"\"\n",
    "        for idx in range(len(transitions)):\n",
    "            transition = transitions[idx]\n",
    "            if transition.reward.size(0) < self.multistep_size:\n",
    "                n_pad_steps = self.multistep_size - transition.reward.size(0)\n",
    "                pad_tuple = (0,0,0,0,0,0,0,n_pad_steps)\n",
    "                curr_state = F.pad(transition.curr_state, pad_tuple, mode='constant', value=0.)\n",
    "                next_state = F.pad(transition.next_state, pad_tuple, mode='constant', value=0.)\n",
    "                pad_tuple = (0,n_pad_steps)\n",
    "                is_done = F.pad(transition.is_done, pad_tuple, mode='constant', value=-1)\n",
    "                action  = F.pad(transition.action,  pad_tuple, mode='constant', value=0)\n",
    "                reward  = F.pad(transition.reward,  pad_tuple, mode='constant', value=0)\n",
    "                transitions[idx] = Transition(curr_state, action, next_state, reward, is_done)\n",
    "    \n",
    "    def train(self, is_terminal_state=False):\n",
    "        if len(self.replay_memory) < self.replay_memory_minlen:\n",
    "            # Start training only if replay memory reached a minimum size\n",
    "            return None\n",
    "        # Get minibatch of multi-step transitions\n",
    "        # transitions is a minibatch_size sized list of Transition\n",
    "        # transitions[i].{curr_state, next_state}  have shape (5, 3, 40, 90)\n",
    "        # transitions[i].{reward, action, is_done} have shape (5,)\n",
    "        transitions = self.replay_memory.sample(\n",
    "                self.minibatch_size, self.multistep_size)\n",
    "        self.__pad_transitions(transitions)\n",
    "        batch = Transition(*zip(*transitions))\n",
    "        # Compute the current Q values\n",
    "        # curr_state has shape (minibatch_size, 5, 3, 40, 90)\n",
    "        curr_states = torch.stack(batch.curr_state)\n",
    "        # curr_Qs has shape (minibatch_size, 5, 2)\n",
    "        curr_Qs = self.policy_net(curr_states, is_optimizing=True)\n",
    "        # Extract the Q values corresponding to agent actions\n",
    "        # actions has shape (minibatch_size, 5, 1)\n",
    "        actions = torch.stack(batch.action).unsqueeze(2)\n",
    "        # curr_Qs, rewards, is_dones have shape (minibatch_size, 5)\n",
    "        curr_Qs = curr_Qs.gather(2, actions).squeeze(2)\n",
    "        rewards = torch.stack(batch.reward)\n",
    "        is_dones = torch.stack(batch.is_done)\n",
    "        # Compute the max of next Q values\n",
    "        next_states = torch.stack(batch.next_state)\n",
    "        with torch.no_grad():\n",
    "            # print(\"next_states has nan\", torch.isnan(next_states).any())\n",
    "            # print(\"next_states\", next_states.min(), next_states.max())\n",
    "            next_Qs = self.target_net(next_states, is_optimizing=True)\n",
    "            # print(\"next_Qs has nan\", torch.isnan(next_Qs).any())\n",
    "        # max_next_Qs has shape (minibatch_size, 5)\n",
    "        max_next_Qs, _ = next_Qs.max(2)\n",
    "        # Compute the expected Q values for TD error\n",
    "        expected_Qs = torch.zeros(\n",
    "                self.minibatch_size, self.multistep_size,\n",
    "                dtype=torch.float, device=device)\n",
    "        mask0 = is_dones == 0\n",
    "        expected_Qs[mask0] = rewards[mask0] + self.discount*max_next_Qs[mask0]\n",
    "        mask1 = is_dones == 1\n",
    "        expected_Qs[mask1] = rewards[mask1]\n",
    "        mask2 = mask0 | mask1\n",
    "        curr_Qs = curr_Qs[mask2]\n",
    "        expected_Qs = expected_Qs[mask2]\n",
    "        # Model training step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(curr_Qs, expected_Qs)\n",
    "        loss.backward()\n",
    "        for param in self.policy_net.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()\n",
    "        # Update target model if necessary\n",
    "        if is_terminal_state:\n",
    "            self.terminal_state_counter += 1\n",
    "        if self.terminal_state_counter > self.update_target_interval:\n",
    "            self.__update_target_model()\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57a422d1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0, device='cuda:0'), 1.0, False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the agent\n",
    "agent = RecurrentStateDQNAgent(env, config, device)\n",
    "agent.reset()\n",
    "curr_state = agent.render(is_input=True)\n",
    "action = agent.select_action(curr_state, is_training=False)\n",
    "_, reward, is_done, _ = env.step(action.item())\n",
    "action, reward, is_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33359acd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on episode 1; epsilon 0.89506235645605\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 5.5\n",
      "    min reward 0\n",
      "    max reward 11.0\n",
      "    latest episode loss 0\n",
      "on episode 50; epsilon 0.5312660701053691\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 21.08\n",
      "    min reward 11.0\n",
      "    max reward 69.0\n",
      "    latest episode loss 0.12339197516441346\n",
      "on episode 100; epsilon 0.28905321140249096\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 24.34\n",
      "    min reward 11.0\n",
      "    max reward 69.0\n",
      "    latest episode loss 0.33056340273469687\n",
      "on episode 150; epsilon 0.15147933906040406\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 25.84\n",
      "    min reward 10.0\n",
      "    max reward 68.0\n",
      "    latest episode loss 0.40847294075148444\n",
      "on episode 200; epsilon 0.07566197193341019\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 27.76\n",
      "    min reward 11.0\n",
      "    max reward 92.0\n",
      "    latest episode loss 0.48610803882280984\n",
      "on episode 250; epsilon 0.05\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 27.82\n",
      "    min reward 13.0\n",
      "    max reward 66.0\n",
      "    latest episode loss 0.6003923364754381\n",
      "on episode 300; epsilon 0.05\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 25.86\n",
      "    min reward 9.0\n",
      "    max reward 60.0\n",
      "    latest episode loss 0.724447508653005\n",
      "on episode 350; epsilon 0.05\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 24.68\n",
      "    min reward 11.0\n",
      "    max reward 52.0\n",
      "    latest episode loss 0.7961151957511902\n",
      "on episode 400; epsilon 0.05\n",
      "    stats aggregated over the last 50 episodes:\n",
      "    avg reward 16.18\n",
      "    min reward 8.0\n",
      "    max reward 36.0\n",
      "    latest episode loss 1.058527252890847\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5485/828200436.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mshould_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshould_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecurrent_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models/pole-dqnet-lstm.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5485/828200436.py\u001b[0m in \u001b[0;36mrecurrent_train\u001b[0;34m(env, config, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                     ))\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_terminal_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_done\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mepisode_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_5485/3048093157.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, is_terminal_state)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 dtype=torch.float, device=device)\n\u001b[1;32m    171\u001b[0m         \u001b[0mmask0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_dones\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mexpected_Qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmax_next_Qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mmask1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_dones\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mexpected_Qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def recurrent_train(env, config, device):\n",
    "    agent = RecurrentStateDQNAgent(env, config, device)\n",
    "    # Reward is 1 for every step taken, including the termination step\n",
    "    episode_rewards = [0]\n",
    "    avgs_reward = []\n",
    "    mins_reward = []\n",
    "    maxs_reward = []\n",
    "    for episode in range(1, config.episodes + 1):\n",
    "        # agent.writer.add_scalar('epsilon', agent.epsilon, episode)\n",
    "        agent.reset()\n",
    "        is_done = False\n",
    "        episode_reward = 0\n",
    "        episode_losses = []\n",
    "        curr_state = agent.render(is_input=True)\n",
    "        next_state = torch.zeros(\n",
    "                curr_state.shape, dtype=curr_state.dtype, device=device)\n",
    "        while not is_done:\n",
    "            action = agent.select_action(curr_state)\n",
    "            _, reward, is_done, _ = env.step(action.item())\n",
    "            episode_reward += reward\n",
    "            if is_done:\n",
    "                next_state = torch.zeros(\n",
    "                        curr_state.shape, dtype=curr_state.dtype, device=device)\n",
    "            else:\n",
    "                next_state = agent.render(is_input=True)\n",
    "            agent.update_replay_memory(\n",
    "                    episode,\n",
    "                    (\n",
    "                        curr_state,\n",
    "                        action,\n",
    "                        next_state,\n",
    "                        torch.tensor(reward, device=device),\n",
    "                        torch.tensor(is_done, dtype=torch.int, device=device)\n",
    "                    ))\n",
    "            loss = agent.train(is_terminal_state=is_done)\n",
    "            if isinstance(loss, numbers.Number):\n",
    "                episode_losses.append(loss)\n",
    "            curr_state = next_state\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        avg_losses = sum(episode_losses) / len(episode_losses) if len(episode_losses) > 0 else 0\n",
    "        # agent.writer.add_scalar('avg_loss', avg_losses, episode)\n",
    "        if episode % config.aggregate_stats_interval == 0 or episode == 1:\n",
    "            episode_rewards = episode_rewards[-config.aggregate_stats_interval:]\n",
    "            avg_reward = sum(episode_rewards) / len(episode_rewards)\n",
    "            min_reward = min(episode_rewards)\n",
    "            max_reward = max(episode_rewards)\n",
    "            # agent.writer.add_scalar('avg_reward', avg_reward, episode)\n",
    "            # agent.writer.add_scalar('min_reward', min_reward, episode)\n",
    "            # agent.writer.add_scalar('max_reward', max_reward, episode)\n",
    "            avgs_reward.append(avg_reward)\n",
    "            mins_reward.append(min_reward)\n",
    "            maxs_reward.append(max_reward)\n",
    "            print(f\"on episode {episode}; epsilon {agent.epsilon}\")\n",
    "            print(f\"    stats aggregated over the last {config.aggregate_stats_interval} episodes:\")\n",
    "            print(f\"    avg reward {avg_reward}\")\n",
    "            print(f\"    min reward {min_reward}\")\n",
    "            print(f\"    max reward {max_reward}\")\n",
    "            print(f\"    latest episode loss {avg_losses}\")\n",
    "            \n",
    "    plt.plot(np.arange(len(avgs_reward)), avgs_reward, label='avg reward')\n",
    "    plt.plot(np.arange(len(mins_reward)), mins_reward, label='min reward')\n",
    "    plt.plot(np.arange(len(maxs_reward)), maxs_reward, label='max reward')\n",
    "    plt.legend()\n",
    "    plt.ylabel(f\"rewards aggregated over the last {config.aggregate_stats_interval} episodes\")\n",
    "    plt.xlabel(f\"episode #\")\n",
    "    # plt.savefig(f\"{agent.log_dir}/training-summary.png\")\n",
    "    return agent\n",
    "\n",
    "\n",
    "should_train = True\n",
    "if should_train:\n",
    "    agent = recurrent_train(env, config, device)\n",
    "    torch.save(agent.policy_net.state_dict(), \"models/pole-dqnet-lstm.pth\")\n",
    "else:\n",
    "    pass\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cba52e90",
   "metadata": {},
   "source": [
    "on episode 1; epsilon 0.89506235645605\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 5.5\n",
    "    min reward 0\n",
    "    max reward 11.0\n",
    "    latest episode loss 0\n",
    "on episode 50; epsilon 0.5312660701053691\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 21.08\n",
    "    min reward 11.0\n",
    "    max reward 69.0\n",
    "    latest episode loss 0.12339197516441346\n",
    "on episode 100; epsilon 0.28905321140249096\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 24.34\n",
    "    min reward 11.0\n",
    "    max reward 69.0\n",
    "    latest episode loss 0.33056340273469687\n",
    "on episode 150; epsilon 0.15147933906040406\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 25.84\n",
    "    min reward 10.0\n",
    "    max reward 68.0\n",
    "    latest episode loss 0.40847294075148444\n",
    "on episode 200; epsilon 0.07566197193341019\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 27.76\n",
    "    min reward 11.0\n",
    "    max reward 92.0\n",
    "    latest episode loss 0.48610803882280984\n",
    "on episode 250; epsilon 0.05\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 27.82\n",
    "    min reward 13.0\n",
    "    max reward 66.0\n",
    "    latest episode loss 0.6003923364754381\n",
    "on episode 300; epsilon 0.05\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 25.86\n",
    "    min reward 9.0\n",
    "    max reward 60.0\n",
    "    latest episode loss 0.724447508653005\n",
    "on episode 350; epsilon 0.05\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 24.68\n",
    "    min reward 11.0\n",
    "    max reward 52.0\n",
    "    latest episode loss 0.7961151957511902\n",
    "on episode 400; epsilon 0.05\n",
    "    stats aggregated over the last 50 episodes:\n",
    "    avg reward 16.18\n",
    "    min reward 8.0\n",
    "    max reward 36.0\n",
    "    latest episode loss 1.058527252890847"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecdc6a",
   "metadata": {},
   "source": [
    "Test the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb4efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
